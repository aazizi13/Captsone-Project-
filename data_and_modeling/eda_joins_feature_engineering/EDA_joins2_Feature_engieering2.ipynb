{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3c3cd2-c6e1-43e9-8f16-ff0499650d58",
   "metadata": {},
   "source": [
    "# Data Wrangling Notebook 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8bdfa2-819d-4ba4-80fd-7f704b660808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import re\n",
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import s3fs\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast, StructType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, split, slice, count, when, expr, isnan, isnull\n",
    "from pyspark.sql.functions import date_format, to_timestamp, concat, unix_timestamp, substring, lit\n",
    "from pyspark.sql.functions import col, month, quarter, dayofweek, year\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "import configparser\n",
    "import findspark\n",
    "import lxml\n",
    "from datetime import timedelta\n",
    "from pandas.tseries.offsets import BDay\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa25a4-0309-4362-a308-2b0d1d50e207",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Reading From S3 Bucket using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db130b-4b36-4d96-87f5-f0eea6ed9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# This code block is used for pulling data from S3 using boto3 library\n",
    "############# Note: Not necessary for this project\n",
    "\n",
    "#getting relevant information for the S3 bucket\n",
    "\n",
    "# AWS_S3_BUCKET='w210-bucket'\n",
    "# AWS_S3_REGION='us-east-1'\n",
    "# AWS_PROFILE_NAME='default'\n",
    "# session = boto3.Session(profile_name=AWS_PROFILE_NAME)\n",
    "# s3 = session.resource('s3')\n",
    "# s3_client = session.client('s3',region_name=AWS_S3_REGION)\n",
    "# my_bucket = s3.Bucket(AWS_S3_BUCKET)\n",
    "\n",
    "# #printing all the files in ridership directory \n",
    "# for objects in my_bucket.objects.filter(Prefix=\"ridership/\"):\n",
    "#     print(objects.key)\n",
    "\n",
    "# #printing all the files in weather directory \n",
    "# for objects in my_bucket.objects.filter(Prefix=\"weather/\"):\n",
    "#     print(objects.key)\n",
    "\n",
    "# #reading one file only as pandas df\n",
    "# obj = s3_client.get_object(Bucket= AWS_S3_BUCKET, Key= \"ridership/date-hour-soo-dest-2011.csv\") \n",
    "# # get object and file (key) from bucket\n",
    "# initial_df = pd.read_csv(obj['Body'], header=None) \n",
    "# initial_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef685327-0b87-4a4e-a1b1-bd046c494607",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reading From S3 Bucket using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35af90ae-44cd-4deb-bcc6-83c480a410cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starting Pyspark Session\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .config('spark.master','local[*]')\\\n",
    "                    .config('spark.add.name','S3app')\\\n",
    "                    .config('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-common:3.3.4')\\\n",
    "                    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8364aa13-912e-4382-a45c-1e2856ee137c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-13-189.ec2.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f07fb6b53d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49bf5dc4-14c5-4b00-924b-28791908fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configuring Pyspark to read data from S3 Bucket. \n",
    "\n",
    "findspark.init()\n",
    "config = configparser.ConfigParser()\n",
    "# AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "# AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "aws_profile = 'default'\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "access_id = config.get(aws_profile, \"aws_access_key_id\") \n",
    "access_key = config.get(aws_profile, \"aws_secret_access_key\")\n",
    "\n",
    "# conf = SparkConf()\n",
    "# conf.set('spark.executor.memory', '2g')\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)\n",
    "# spark.conf.set(\"spark.sql.broadcastTimeout\", 3000)\n",
    "\n",
    "# spark.conf.set('spark.executor.memory', '4G')\n",
    "# spark.conf.set('spark.driver.memory', '16G')\n",
    "# spark.conf.set('spark.driver.maxResultSize', '10G')\n",
    "\n",
    "# spark.conf.set(\"spark.driver.maxResultSize\", \"8g\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.committer.name\",\"magic\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.awsAccessKeyId\", access_id)\n",
    "hadoop_conf.set(\"fs.s3a.awsSecretAccessKey\", access_key)\n",
    "hadoop_conf.set('spark.sql.files.maxPartitionBytes','134217728')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa361767-9a08-4ec4-9f0a-119bed90acdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sports Event Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549a440-2661-4ac8-b145-568a6ec34447",
   "metadata": {
    "tags": []
   },
   "source": [
    "### In this section of Notebook, we will read the Sports Events data and do data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e78434-36ec-4990-90ae-58516e96a704",
   "metadata": {},
   "source": [
    "We will use webscrapping to grab the necessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "648fdcaf-d50b-4765-bac4-8f93b9e49f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Note: This block has been commented out, the webscrapping has already been completed and added to S3 bucket. ########\n",
    "\n",
    "# # mapping dates\n",
    "# days = {'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4,'May':5,'Jun':6, \n",
    "#          'Jul':7, 'Aug':8, 'Sep':9,'Oct':10, 'Nov':11, 'Dec':12}\n",
    "\n",
    "# # generate input dates\n",
    "# date_range = pd.date_range('2011-01-01', '2022-12-31', freq='D')\n",
    "# date_range = pd.Series(date_range)\n",
    "# date_range = pd.to_datetime(date_range)\n",
    "# date_range_list = date_range.dt.strftime('%Y/%m/%d').tolist()\n",
    "# date_range_list.sort()\n",
    "# input_dates = date_range_list\n",
    "\n",
    "# # input_dates = ['2022/02/27','2022/03/02','2022/11/01','2022/11/04','2022/11/05']\n",
    "\n",
    "# # starting an empty dataframe\n",
    "# final_df = pd.DataFrame()\n",
    "\n",
    "# #web scrapping the relevant information\n",
    "\n",
    "# # getting data for each date\n",
    "# for input_date in input_dates:\n",
    "#     str = 'https://dothebay.com/events/sports/{}'\n",
    "#     url = str.format(input_date)\n",
    "#     page = requests.get(url)\n",
    "#     soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "    \n",
    "#     # starting empty lists\n",
    "#     event_venues = []\n",
    "#     venue_location = []\n",
    "#     event_ts = []\n",
    "#     latitude = []\n",
    "#     longitude = []\n",
    "    \n",
    "#     overall_groups = soup.find('div', attrs={'class','ds-listings ds-listings-list'})\n",
    "#     if overall_groups is None:\n",
    "#         continue\n",
    "#     if overall_groups.find('div', attrs={'class','ds-events-page'}) is None:\n",
    "#         continue \n",
    "#     item = soup.find('div', attrs={'class': 'ds-events-group'})\n",
    "#     if item is None:\n",
    "#         continue\n",
    "#     upcoming = soup.find('div', attrs={'class': 'upcoming-event-groups'})\n",
    "#     if upcoming is None:\n",
    "#         continue\n",
    "     \n",
    "#     #getting the venue names\n",
    "#     for i in item.find_all('div', attrs={'class': 'ds-venue-name'}):\n",
    "#         event_venues.append(i.text)\n",
    "#         venue_names = [i.strip() for i in event_venues]\n",
    "        \n",
    "#     #getting the timestamps\n",
    "#     for t in item.find_all('meta', attrs={'itemprop':'startDate'}):\n",
    "#         time = t.get('content')\n",
    "#         event_ts.append(time)\n",
    "        \n",
    "    \n",
    "#     #getting the geo_locations    \n",
    "#     for l in item.find_all('span',attrs={'itemprop':'geo'}):\n",
    "#         for lat in l.find_all('meta', attrs={'itemprop':'latitude'}):\n",
    "#             lati = lat.get('content')\n",
    "#             latitude.append(lati)\n",
    "#         for lon in l.find_all('meta', attrs={'itemprop':'longitude'}):\n",
    "#             long = lon.get('content')\n",
    "#             longitude.append(long)\n",
    "            \n",
    "#     #generating a dataframe\n",
    "#     venue_names = pd.DataFrame(venue_names, columns=['venue_name'])\n",
    "#     latitude = pd.DataFrame(latitude, columns=['latitude'])\n",
    "#     longitude = pd.DataFrame(longitude, columns=['longitude'])\n",
    "#     time = pd.DataFrame(event_ts, columns=['ts'])\n",
    "\n",
    "#     df_1 = pd.concat([time,venue_names,latitude,longitude,], axis = 1)\n",
    "#     final_df = pd.concat([final_df,df_1], axis =0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "699e4985-af69-4191-bc39-35def970a72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------+---------------+\n",
      "|                 ts|          venue_name|latitude_venue|longitude_venue|\n",
      "+-------------------+--------------------+--------------+---------------+\n",
      "|2013-02-10 19:00:00|Event Center at SJSU|     37.335079|    -121.880754|\n",
      "|2013-01-16 19:00:00|          Cow Palace|     37.706765|    -122.418738|\n",
      "|2015-07-05 13:00:00|Oakland-Alameda C...|     37.750342|    -122.202805|\n",
      "|2012-03-24 19:00:00|          SAP Center|     37.332240|    -121.901650|\n",
      "|2012-11-16 19:00:00|          Cow Palace|     37.706765|    -122.418738|\n",
      "|2013-02-15 19:00:00|Event Center at SJSU|     37.335079|    -121.880754|\n",
      "|2014-01-22 19:00:00|          SAP Center|     37.332240|    -121.901650|\n",
      "|2013-01-29 19:00:00|          SAP Center|     37.332240|    -121.901650|\n",
      "|2012-10-03 19:00:00|          SAP Center|     37.332240|    -121.901650|\n",
      "|2013-01-31 19:00:00|          SAP Center|     37.332240|    -121.901650|\n",
      "|2014-03-23 17:00:00|          SAP Center|     37.332240|    -121.901650|\n",
      "|2015-07-01 12:00:00|Oakland-Alameda C...|     37.750342|    -122.202805|\n",
      "|2012-02-02 19:00:00|          SAP Center|     37.332240|    -121.901650|\n",
      "|2014-06-28 19:00:00|          SAP Center|     37.332240|    -121.901650|\n",
      "|2013-01-26 19:00:00|Event Center at SJSU|     37.335079|    -121.880754|\n",
      "|2012-10-12 19:00:00|          Cow Palace|     37.706765|    -122.418738|\n",
      "|2011-12-18 19:00:00|          SAP Center|     37.332240|    -121.901650|\n",
      "|2012-12-13 19:00:00|          Cow Palace|     37.706765|    -122.418738|\n",
      "|2013-01-13 14:00:00|          Cow Palace|     37.706765|    -122.418738|\n",
      "|2013-05-25 19:00:00|          SAP Center|     37.332240|    -121.901650|\n",
      "+-------------------+--------------------+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- venue_name: string (nullable = true)\n",
      " |-- latitude_venue: string (nullable = true)\n",
      " |-- longitude_venue: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2940"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### read the sporting events data. \n",
    "\n",
    "#read to pandas\n",
    "sports = pd.read_excel('s3a://w210-bucket/sports_events_full.xlsx')\n",
    "sports_1 = sports.dropna(subset = ['ts'])\n",
    "\n",
    "# Filling up the missing longitude and latitude data\n",
    "# getting rid of NAs\n",
    "sports_2 = sports_1.drop(columns=['ts']) \\\n",
    "                   .dropna() \\\n",
    "                   .drop_duplicates(subset=['venue_name'], keep='first', inplace=False, ignore_index=True)\n",
    "\n",
    "# sports_2 = sports_2.dropna()\n",
    "# sports_2 = sports_2.drop_duplicates(subset=['venue_name'], keep='first', inplace=False, ignore_index=True)\n",
    "sports_3 = pd.merge(sports_1, sports_2, on = 'venue_name', how ='outer')\n",
    "\n",
    "sports_4 = sports_3.drop(columns=['latitude_x','longitude_x']) \\\n",
    "                   .drop(columns=['ts']) \\\n",
    "                   .drop_duplicates(subset=['venue_name'], keep='first', inplace=False, ignore_index=True) \\\n",
    "                   .rename(columns={\"latitude_y\": \"latitude\", \"longitude_y\": \"longitude\"})\n",
    "\n",
    "data = {'venue_name': ['Civic Center Plaza','Civic Center','Downtown Oakland','Spartan Stadium','Laird Q. Cagan Stadium',\n",
    "                       'UCB Edwards Stadium','PayPal Park', 'AT&T Park',\n",
    "                       ],\n",
    "        'latitude': [37.7801340,37.7799730,37.8043549,37.3197651,37.4332795,37.8691258,37.3513242,37.3350794],\n",
    "        'longitude': [-122.4177319,-122.4187285,-122.2710607,-121.8682858,-122.1580094,-122.2648963,-121.9245659,-121.880754]}\n",
    "missing_geos =pd.DataFrame(data)\n",
    "\n",
    "sports_5 = pd.concat([sports_4,missing_geos]) \\\n",
    "             .dropna()\n",
    "\n",
    "final_sports = pd.merge(sports_1,sports_5, on = 'venue_name', how = 'outer' )\n",
    "final_sports = final_sports.drop(columns=['latitude_x','longitude_x'])\\\n",
    "                           .rename(columns={\"latitude_y\": \"latitude\", \"longitude_y\": \"longitude\"}) \\\n",
    "                           .dropna()\n",
    "\n",
    "# #convert to PySpark Dataframe\n",
    "mySchema = StructType([ StructField(\"ts\", StringType(), True)\\\n",
    "                       ,StructField(\"venue_name\", StringType(), True) \\\n",
    "                       ,StructField(\"latitude\", StringType(), True) \\\n",
    "                       ,StructField(\"longitude\", StringType(), True)])\n",
    "df_sports = spark.createDataFrame(final_sports, schema=mySchema)\n",
    "\n",
    "#convert to timestamp\n",
    "df_sports = df_sports.withColumn(\"ts\", concat(substring(col(\"ts\"), 1, 10), lit(' '), substring(col(\"ts\"),12 , 2), lit(':00')))\n",
    "df_sports = df_sports.withColumn(\"ts\",f.to_timestamp(col('ts')))\n",
    "\n",
    "#rename venue latitude and longitude\n",
    "df_sports = df_sports.withColumnRenamed('latitude','latitude_venue') \\\n",
    "                     .withColumnRenamed('longitude','longitude_venue')\n",
    "df_sports = df_sports.orderBy('ts')\n",
    "df_sports = df_sports.withColumn(\"longitude_venue\", concat(substring(col(\"longitude_venue\"), 1, 11)))\n",
    "df_sports = df_sports.withColumn(\"latitude_venue\", concat(substring(col(\"latitude_venue\"), 1, 9)))\n",
    "\n",
    "# filtering venues\n",
    "df_sports_filtered = df_sports.toPandas()\n",
    "df_sports_filtered=df_sports_filtered.replace('Avaya Stadium (SJ)','PayPal Park')\n",
    "df_sports_filtered=df_sports_filtered.replace('AT&T Park','Oracle Park')\n",
    "df_sports_filtered = df_sports_filtered[df_sports_filtered.groupby(\"venue_name\")[\"venue_name\"].transform('count') > 4]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Era Art Bar and Lounge'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != '4th Street Pizza Company'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Sweetwater Music Hall'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Trademark Sports Bar & Eatery'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Madrone Art Bar'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Trademark and Copyright'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Hawthorn'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Jaxson'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Harding Park'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Oakland Technical High School'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Spark Social SF'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'The Phoenix Theater (Petaluma)'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Yellowjacket Stadium'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Golden 1 Center'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'District Six'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'The New Parkway'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Candlestick Park'\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "filtered = df_sports_filtered['venue_name'] != 'Kaiser Permanente Arena'\n",
    "\n",
    "\n",
    "\n",
    "df_sports_filtered = df_sports_filtered[filtered]\n",
    "sports_filtered = spark.createDataFrame(df_sports_filtered)\n",
    "sports_filtered = sports_filtered.dropDuplicates()\n",
    "\n",
    "sports_filtered.show()\n",
    "sports_filtered.printSchema()\n",
    "sports_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d1a5885-1964-41db-af76-ab146f63b9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+--------------+---------------+\n",
      "|venue_name                     |latitude_venue|longitude_venue|\n",
      "+-------------------------------+--------------+---------------+\n",
      "|SAP Center                     |37.332240     |-121.901650    |\n",
      "|Kezar Stadium                  |37.766948     |-122.456000    |\n",
      "|Event Center at SJSU           |37.335079     |-121.880754    |\n",
      "|Oakland-Alameda County Coliseum|37.750342     |-122.202805    |\n",
      "|Oracle Park                    |37.335079     |-121.880754    |\n",
      "|Cow Palace                     |37.706765     |-122.418738    |\n",
      "|Oracle Park                    |37.778419     |-122.390621    |\n",
      "|The Chapel                     |37.760565     |-122.421188    |\n",
      "|Levi's Stadium                 |37.402317     |-121.968995    |\n",
      "|PayPal Park                    |37.351573     |-121.925482    |\n",
      "|Stanford Stadium               |37.434529     |-122.161122    |\n",
      "|Spartan Stadium                |37.319765     |-121.868285    |\n",
      "|Chase Center                   |37.768829     |-122.389416    |\n",
      "|Historic BAL Theatre           |37.787992     |-122.404294    |\n",
      "|PayPal Park                    |37.351324     |-121.924565    |\n",
      "+-------------------------------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sports_filtered.select('venue_name', 'latitude_venue', 'longitude_venue').distinct().show(50,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c361241f-e911-488d-8ba7-76f8ad6e5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_filtered = sports_filtered.withColumn(\"latitude_venue\", \\\n",
    "              when(sports_filtered[\"venue_name\"] == 'Event Center at SJSU', 37.335366).otherwise(sports_filtered[\"latitude_venue\"]))\n",
    "sports_filtered = sports_filtered.withColumn(\"longitude_venue\", \\\n",
    "              when(sports_filtered[\"venue_name\"] == 'Event Center at SJSU', -121.881039).otherwise(sports_filtered[\"longitude_venue\"]))\n",
    "# 37.3353665202424, -121.8810393157378\n",
    "\n",
    "sports_filtered = sports_filtered.withColumn(\"latitude_venue\", \\\n",
    "              when(sports_filtered[\"venue_name\"] == 'Oracle Park', 37.778832).otherwise(sports_filtered[\"latitude_venue\"]))\n",
    "sports_filtered = sports_filtered.withColumn(\"longitude_venue\", \\\n",
    "              when(sports_filtered[\"venue_name\"] == 'Oracle Park', -122.389344).otherwise(sports_filtered[\"longitude_venue\"]))\n",
    "# 37.77883251646694, -122.38934490408342\n",
    "\n",
    "sports_filtered = sports_filtered.withColumn(\"latitude_venue\", \\\n",
    "              when(sports_filtered[\"venue_name\"] == 'Cow Palace', 37.707880).otherwise(sports_filtered[\"latitude_venue\"]))\n",
    "sports_filtered = sports_filtered.withColumn(\"longitude_venue\", \\\n",
    "              when(sports_filtered[\"venue_name\"] == 'Cow Palace', -122.420308).otherwise(sports_filtered[\"longitude_venue\"]))\n",
    "# 37.70788067558349, -122.42030831580716\n",
    "\n",
    "sports_filtered = sports_filtered.withColumn(\"latitude_venue\", \\\n",
    "              when(sports_filtered[\"venue_name\"] == 'Historic BAL Theatre', 37.708800).otherwise(sports_filtered[\"latitude_venue\"]))\n",
    "sports_filtered = sports_filtered.withColumn(\"longitude_venue\", \\\n",
    "              when(sports_filtered[\"venue_name\"] == 'Historic BAL Theatre', -122.133087).otherwise(sports_filtered[\"longitude_venue\"]))\n",
    "\n",
    "# 37.708800133893575, -122.1330872483417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6c99fa0-6f28-4cca-896f-1b3d2df63e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abbreviation: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      "\n",
      "+------------+-----------------------------------+-----------+---------+\n",
      "|abbreviation|station_name                       |longitude  |latitude |\n",
      "+------------+-----------------------------------+-----------+---------+\n",
      "|12TH        |12th St. Oakland City Center (12TH)|-122.271450|37.803768|\n",
      "|16TH        |16th St. Mission (16TH)            |-122.419694|37.765062|\n",
      "|19TH        |19th St. Oakland (19TH)            |-122.268602|37.808350|\n",
      "|24TH        |24th St. Mission (24TH)            |-122.418143|37.752470|\n",
      "|ASHB        |Ashby (ASHB)                       |-122.270062|37.852803|\n",
      "+------------+-----------------------------------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### read the station data to get station abbreviations, Longitude, and Latitude\n",
    "\n",
    "df_station = spark.read.option(\"header\",True) \\\n",
    "           .csv(\"s3a://w210-bucket/ridership/station_bart.csv\")\n",
    "\n",
    "#rename columns\n",
    "df_station = df_station.withColumnRenamed('Abbreviation','abbreviation') \\\n",
    "                        .withColumnRenamed('Location','location')\\\n",
    "                        .withColumnRenamed('Name','station_name').drop('description').drop('name')\n",
    "\n",
    "#split longitude and latitude into separate columns\n",
    "df_station = df_station.withColumn(\"longitude\", split(col(\"location\"), \",\").getItem(0)) \\\n",
    "                         .withColumn(\"latitude\", split(col(\"location\"), \",\").getItem(1)).drop('location')\n",
    "\n",
    "# adding the missing station data to the df_station data\n",
    "columns = ['abbreviation', 'station_name', 'longitude', 'latitude']\n",
    "data = [\\\n",
    "        ('ANTC','Antioch Station', '-121.780320', '37.996012'),\\\n",
    "        ('PCTR','Pittsburg Center Station', '-121.888538', '38.018200'),\\\n",
    "        ('BERY','Berryessa Bart Station', '-121.874689', '37.368572'),\\\n",
    "        ('MLPT','Milpitas Bart Station', '-121.890621', '37.409878')]\n",
    "\n",
    "missing_stations = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_station_full = df_station.union(missing_stations)\n",
    "df_station_full.printSchema()\n",
    "df_station_full.show(5, truncate = False)\n",
    "df_station_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e7af6-9c32-4158-8f2f-66132ca6bf9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "1c3a426a-3146-4dc3-a696-3f1f7a9b356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate all the sports venue within a 3 mile radius\n",
    "# from geopy.distance import geodesic\n",
    "\n",
    "# @f.udf(returnType=FloatType())\n",
    "\n",
    "# def geodesic_udf(a, b):\n",
    "#     return geodesic(a, b).km*0.62\n",
    "\n",
    "# sports_filtered1= sports_filtered.drop('ts')\n",
    "# venue_bart_joined = sports_filtered1.join(df_station_full) \\\n",
    "#                                .withColumn('distance_mile', geodesic_udf(f.array('latitude','longitude'),\n",
    "#                                                                      f.array('latitude_venue','longitude_venue'))).drop('station_name')\n",
    "\n",
    "# venue_bart_joind_3_miles = venue_bart_joined.filter(col(\"distance_mile\") <= 4).drop('longitude','latitude').dropDuplicates()\n",
    "# venue_bart_joind_3_miles.show(5)\n",
    "# venue_bart_joind_3_miles.printSchema()\n",
    "# # venue_bart_joind_3_miles.count()                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "ca3d41f7-4981-45cb-a286-639cee72ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving venue_bart_joind_3_miles to a parquet file \n",
    "# venue_bart_joind_3_miles.write.parquet('s3a://w210-bucket/data_wrangling/venue_bart_joind_3_miles.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cb268a6-5334-4b8f-8a7c-7fa1228554e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- venue_name: string (nullable = true)\n",
      " |-- latitude_venue: string (nullable = true)\n",
      " |-- longitude_venue: string (nullable = true)\n",
      " |-- abbreviation: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- venue_min_distance: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2621"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alternatively, calculate the closest distance of each bart station to a sports venue\n",
    "\n",
    "# station_weather_joined to the station data\n",
    "sports_filtered1 = sports_filtered.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "#parititioning by index \n",
    "w=Window().partitionBy(\"index\")\n",
    "\n",
    "#calculating the Euclidean distance between Bart stations and weather stations\n",
    "#filtering by min distance from each bart station to weather stations \n",
    "bart_venue_min_distance = sports_filtered1.join(df_station_full) \\\n",
    "            .withColumn(\"venue_distance\",f.sqrt(f.pow(f.col(\"latitude\")-f.col(\"latitude_venue\"),2)+\\\n",
    "                                   f.pow(f.col(\"longitude\")-f.col(\"longitude_venue\"),2)))\\\n",
    "            .withColumn(\"venue_min_distance\", f.min(\"venue_distance\").over(w))\\\n",
    "            .filter('venue_distance=venue_min_distance') \\\n",
    "            .drop('station_name','index','venue_distance')\n",
    "bart_venue_min_distance = bart_venue_min_distance.dropDuplicates()\n",
    "\n",
    "# fixing the error \n",
    "bart_venue_min_distance = bart_venue_min_distance.withColumn(\"abbreviation\", \\\n",
    "              when(bart_venue_min_distance[\"venue_name\"] == 'Cow Palace', 'BALB').otherwise(bart_venue_min_distance[\"abbreviation\"]))\n",
    "bart_venue_min_distance = bart_venue_min_distance.withColumn(\"longitude\", \\\n",
    "              when(bart_venue_min_distance[\"venue_name\"] == 'BALB', -122.447506).otherwise(bart_venue_min_distance[\"longitude\"]))\n",
    "bart_venue_min_distance = bart_venue_min_distance.withColumn(\"latitude\", \\\n",
    "              when(bart_venue_min_distance[\"venue_name\"] == 'BALB', 37.721585).otherwise(bart_venue_min_distance[\"latitude\"]))\n",
    "\n",
    "bart_venue_min_distance = bart_venue_min_distance.withColumn(\"abbreviation\", \\\n",
    "              when(bart_venue_min_distance[\"venue_name\"] == 'Chase Center', '16TH').otherwise(bart_venue_min_distance[\"abbreviation\"]))\n",
    "bart_venue_min_distance = bart_venue_min_distance.withColumn(\"longitude\", \\\n",
    "              when(bart_venue_min_distance[\"venue_name\"] == '16TH', -122.419694).otherwise(bart_venue_min_distance[\"longitude\"]))\n",
    "bart_venue_min_distance = bart_venue_min_distance.withColumn(\"latitude\", \\\n",
    "              when(bart_venue_min_distance[\"venue_name\"] == '16TH', 37.765062).otherwise(bart_venue_min_distance[\"latitude\"]))\n",
    "\n",
    "# 37.76817935620899, -122.3878343023004\n",
    "\n",
    "bart_venue_min_distance.printSchema()\n",
    "bart_venue_min_distance.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c95bf174-c6c7-4482-922b-5d5abfd936eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|          venue_name|abbreviation|\n",
      "+--------------------+------------+\n",
      "|Historic BAL Theatre|        BAYF|\n",
      "|Oakland-Alameda C...|        COLS|\n",
      "|        Chase Center|        16TH|\n",
      "|    Stanford Stadium|        UCTY|\n",
      "|          Cow Palace|        BALB|\n",
      "|          SAP Center|        BERY|\n",
      "|         Oracle Park|        MONT|\n",
      "|      Levi's Stadium|        MLPT|\n",
      "|         PayPal Park|        BERY|\n",
      "|       Kezar Stadium|        16TH|\n",
      "|Event Center at SJSU|        BERY|\n",
      "|     Spartan Stadium|        BERY|\n",
      "|          The Chapel|        16TH|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bart_venue_min_distance.select('venue_name','abbreviation').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44925a79-d41b-4df4-842c-8504148a905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving bart_venue_min_distance to a parquet file in S3 bucket\n",
    "# bart_venue_min_distance.write.parquet('s3a://w210-bucket/data_wrangling/bart_venue_min_distance.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b7e71-da22-4dc0-9db7-6de5cb27e43f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Joining Sporting Events Dataset and Bart Station Dataset Based on 3 mile radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5182ff7-eff7-4cce-aa62-f1126defbedf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: s3a://w210-bucket/data_wrangling/venue_bart_joind_3_miles.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4562/1693736954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reading the venue_bart_joind_3_miles date set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvenue_bart_joind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://w210-bucket/data_wrangling/venue_bart_joind_3_miles.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvenue_bart_joind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvenue_bart_joind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    362\u001b[0m         )\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     def text(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: s3a://w210-bucket/data_wrangling/venue_bart_joind_3_miles.parquet"
     ]
    }
   ],
   "source": [
    "# reading the venue_bart_joind_3_miles date set \n",
    "venue_bart_joind = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/venue_bart_joind_3_miles.parquet\")\n",
    "venue_bart_joind.printSchema()\n",
    "venue_bart_joind.count()\n",
    "\n",
    "#converting venue_bart_joind to pandas dataframe\n",
    "df_venue_bart = venue_bart_joind.toPandas()\n",
    "df_venue_bart = df_venue_bart.sort_values('abbreviation')\n",
    "df_venue_bart1 = df_venue_bart.drop(columns=['latitude_venue','longitude_venue','distance_mile'])\n",
    "\n",
    "# get unique bart station abbreviations\n",
    "unique_bart_stations = []\n",
    "for bart in df_venue_bart1['abbreviation']:\n",
    "    unique_bart_stations.append(bart)\n",
    "unique_bart_stations = list(set(unique_bart_stations))\n",
    "\n",
    "new_list = list(enumerate(unique_bart_stations,1))\n",
    "df_test = pd.DataFrame(new_list,columns=['bart_station_id','abbreviation'])\n",
    "\n",
    "df_venue_bart1 = pd.merge(df_venue_bart1, df_test, on = 'abbreviation',how = 'left')\n",
    "df_venue_bart2 = spark.createDataFrame(df_venue_bart1) \n",
    "\n",
    "# joining \n",
    "df_sports_id = df_sports.join(df_venue_bart2, 'venue_name', how='left')\n",
    "# df_sports_id.show()\n",
    "\n",
    "#drop NAs\n",
    "df_sports_id2 = df_sports_id.na.drop().drop('latitude_venue','longitude_venue')\n",
    "# df_sports_id2.show()\n",
    "\n",
    "# reading df_join1 \n",
    "df_join2 = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_joined2.parquet\")\n",
    "# df_join1.printSchema()\n",
    "\n",
    "# get unique bart station abbreviations\n",
    "unique_bart_stations = []\n",
    "for bart in df_venue_bart1['abbreviation']:\n",
    "    unique_bart_stations.append(bart)\n",
    "unique_bart_stations = list(set(unique_bart_stations))\n",
    "\n",
    "new_list = list(enumerate(unique_bart_stations,1))\n",
    "df_test = pd.DataFrame(new_list,columns=['bart_station_id','abbreviation'])\n",
    "bart_id = spark.createDataFrame(df_test)\n",
    "\n",
    "df_join3 = df_join2.join(bart_id, df_join2.origin == bart_id.abbreviation, how='left').drop('abbreviation')\n",
    "df_join3.select('origin','bart_station_id').show(5)\n",
    "\n",
    "df_sports_id_1 = df_sports_id.withColumn('bart_station_id',col('bart_station_id').cast(StringType()))\n",
    "df_sports_id_1 = df_sports_id_1.withColumnRenamed('bart_station_id','bart_station_id_1')\n",
    "df_sports_id_1 = df_sports_id_1.withColumnRenamed('ts','ts1')\n",
    "\n",
    "df_sports_id_2 = df_sports_id_1.na.drop()\n",
    "\n",
    "# joining df_join2 data to the df_sport data on venue_name and ts \n",
    "\n",
    "df_joined3 = df_join3.join(df_sports_id_2, (df_join3['ts'] == df_sports_id_2['ts1']) &\n",
    "                                    (df_join3['bart_station_id'] == df_sports_id_2['bart_station_id_1']),'left').drop('bart_station_id_1','ts1','abbreviation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "973ee3db-0c87-4c15-9dfa-42a29750847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to S3 bucket as parquet file\n",
    "# df_joined3.write.parquet('s3a://w210-bucket/data_wrangling/df_joined3.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "5706754a-8c92-4bac-b111-fffe01b9c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_joined3.parquet\")\n",
    "\n",
    "# renaming events columns \n",
    "df_4 = df_3.withColumnRenamed('bart_station_id','bart_station_id_origin')\\\n",
    "           .withColumnRenamed('venue_name','venue_name_origin')\\\n",
    "           .withColumnRenamed('latitude_venue','latitude_venue_origin')\\\n",
    "           .withColumnRenamed('longitude_venue','longitude_venue_origin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab4201-074e-4650-b5ba-3dea91bed73a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## joining based on destination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "ca311129-21be-4294-afc4-7c3a0ed19888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = df_4.join(bart_id, df_4.destination == bart_id.abbreviation, how='left').drop('abbreviation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "2c1ba27e-ab4c-40a1-9b25-4003450a5773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      " |-- ridership_number: integer (nullable = true)\n",
      " |-- origin-des: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- quarter: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- STATION_origin: string (nullable = true)\n",
      " |-- SOURCE_origin: string (nullable = true)\n",
      " |-- latitude_wthr_origin: string (nullable = true)\n",
      " |-- longitude_wthr_origin: string (nullable = true)\n",
      " |-- ELEVATION_origin: string (nullable = true)\n",
      " |-- NAME_origin: string (nullable = true)\n",
      " |-- REPORT_TYPE_origin: string (nullable = true)\n",
      " |-- CALL_SIGN_origin: string (nullable = true)\n",
      " |-- QUALITY_CONTROL_origin: string (nullable = true)\n",
      " |-- WND_origin: string (nullable = true)\n",
      " |-- CIG_origin: string (nullable = true)\n",
      " |-- VIS_origin: string (nullable = true)\n",
      " |-- TMP_origin: string (nullable = true)\n",
      " |-- DEW_origin: string (nullable = true)\n",
      " |-- SLP_origin: string (nullable = true)\n",
      " |-- AA1_origin: string (nullable = true)\n",
      " |-- AT1_origin: string (nullable = true)\n",
      " |-- distance_origin: double (nullable = true)\n",
      " |-- min_distance_origin: double (nullable = true)\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- latitude_wthr: string (nullable = true)\n",
      " |-- longitude_wthr: string (nullable = true)\n",
      " |-- ELEVATION: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- REPORT_TYPE: string (nullable = true)\n",
      " |-- CALL_SIGN: string (nullable = true)\n",
      " |-- QUALITY_CONTROL: string (nullable = true)\n",
      " |-- WND: string (nullable = true)\n",
      " |-- CIG: string (nullable = true)\n",
      " |-- VIS: string (nullable = true)\n",
      " |-- TMP: string (nullable = true)\n",
      " |-- DEW: string (nullable = true)\n",
      " |-- SLP: string (nullable = true)\n",
      " |-- AA1: string (nullable = true)\n",
      " |-- AT1: string (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- min_distance: double (nullable = true)\n",
      " |-- bart_station_id_origin: long (nullable = true)\n",
      " |-- venue_name_origin: string (nullable = true)\n",
      " |-- latitude_venue_origin: string (nullable = true)\n",
      " |-- longitude_venue_origin: string (nullable = true)\n",
      " |-- bart_station_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "735c967a-77b8-442a-95ca-ddcb17ce79cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- venue_name: string (nullable = true)\n",
      " |-- ts1: timestamp (nullable = true)\n",
      " |-- latitude_venue: string (nullable = true)\n",
      " |-- longitude_venue: string (nullable = true)\n",
      " |-- abbreviation: string (nullable = true)\n",
      " |-- bart_station_id_1: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8043"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sports_id_1 = df_sports_id.withColumn('bart_station_id',col('bart_station_id').cast(StringType()))\n",
    "df_sports_id_1 = df_sports_id_1.withColumnRenamed('bart_station_id','bart_station_id_1')\n",
    "df_sports_id_1 = df_sports_id_1.withColumnRenamed('ts','ts1')\n",
    "df_sports_id_2 = df_sports_id_1.na.drop()\n",
    "df_sports_id_2.printSchema()\n",
    "df_sports_id_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "5d918fa8-ddf1-47b5-b1ca-9f0e8d81cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining df_join3 data to the df_sport data on venue_name and ts \n",
    "\n",
    "df_joined4 = df_5.join(df_sports_id_2, (df_5['ts'] == df_sports_id_2['ts1']) &\n",
    "                                    (df_5['bart_station_id'] == df_sports_id_2['bart_station_id_1']),'left').drop('bart_station_id_1','ts1','abbreviation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "e2b89374-170f-4fd6-b338-e0c3a1d539eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to S3 bucket as parquet file\n",
    "# df_joined4.write.parquet('s3a://w210-bucket/data_wrangling/df_joined4.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ba4ab9-43e1-4476-9c84-d93749ff7f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 04:21:45 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_4 = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_joined4.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7825f29d-7243-487f-ba2a-d6d6f88d296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      " |-- ridership_number: integer (nullable = true)\n",
      " |-- origin-des: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- quarter: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- STATION_origin: string (nullable = true)\n",
      " |-- SOURCE_origin: string (nullable = true)\n",
      " |-- latitude_wthr_origin: string (nullable = true)\n",
      " |-- longitude_wthr_origin: string (nullable = true)\n",
      " |-- ELEVATION_origin: string (nullable = true)\n",
      " |-- NAME_origin: string (nullable = true)\n",
      " |-- REPORT_TYPE_origin: string (nullable = true)\n",
      " |-- CALL_SIGN_origin: string (nullable = true)\n",
      " |-- QUALITY_CONTROL_origin: string (nullable = true)\n",
      " |-- WND_origin: string (nullable = true)\n",
      " |-- CIG_origin: string (nullable = true)\n",
      " |-- VIS_origin: string (nullable = true)\n",
      " |-- TMP_origin: string (nullable = true)\n",
      " |-- DEW_origin: string (nullable = true)\n",
      " |-- SLP_origin: string (nullable = true)\n",
      " |-- AA1_origin: string (nullable = true)\n",
      " |-- AT1_origin: string (nullable = true)\n",
      " |-- distance_origin: double (nullable = true)\n",
      " |-- min_distance_origin: double (nullable = true)\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- latitude_wthr: string (nullable = true)\n",
      " |-- longitude_wthr: string (nullable = true)\n",
      " |-- ELEVATION: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- REPORT_TYPE: string (nullable = true)\n",
      " |-- CALL_SIGN: string (nullable = true)\n",
      " |-- QUALITY_CONTROL: string (nullable = true)\n",
      " |-- WND: string (nullable = true)\n",
      " |-- CIG: string (nullable = true)\n",
      " |-- VIS: string (nullable = true)\n",
      " |-- TMP: string (nullable = true)\n",
      " |-- DEW: string (nullable = true)\n",
      " |-- SLP: string (nullable = true)\n",
      " |-- AA1: string (nullable = true)\n",
      " |-- AT1: string (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- min_distance: double (nullable = true)\n",
      " |-- bart_station_id_origin: long (nullable = true)\n",
      " |-- venue_name_origin: string (nullable = true)\n",
      " |-- latitude_venue_origin: string (nullable = true)\n",
      " |-- longitude_venue_origin: string (nullable = true)\n",
      " |-- bart_station_id: long (nullable = true)\n",
      " |-- venue_name: string (nullable = true)\n",
      " |-- latitude_venue: string (nullable = true)\n",
      " |-- longitude_venue: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109769289"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4.printSchema()\n",
    "df_4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42b6d4e-0f31-458f-9aa0-526bd9b71502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_4.select('ts','origin','destination','ridership_number','venue_name_origin','bart_station_id','venue_name').show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bede5-cfb0-4cf6-adeb-c66c5ca5b8fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Joining Sporting Events Dataset and Bart Station Dataset Based on minimum distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "603a4e6b-bd95-4dce-a155-cab7218f4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the df_join2 and the bart_venue_min_distance\n",
    "df_join2_1 = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_joined2_1.parquet\")\n",
    "bart_venue_min_distance = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/bart_venue_min_distance.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff467828-6479-48c4-a7e0-fc5d6ee5505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will look at two hours before the event time. We will wait 3 hours for the event. And then consider one hour after the event \n",
    "sports_filtered = sports_filtered.drop('latitude_venue','longitude_venue')\n",
    "final_venue_df = sports_filtered.join(bart_venue_min_distance, ['ts','venue_name'],'right').drop('venue_min_distance','longitude','latitude')\n",
    "final_venue_df= final_venue_df.dropDuplicates()\n",
    "final_venue_df_1 = final_venue_df.withColumn('venue_name',f.when(f.col(\"venue_name\").isNull(),f.lit(0)).otherwise(f.lit(1)))\\\n",
    "           .withColumn('venue_name',col('venue_name').cast('string'))\n",
    "\n",
    "ts_extract = final_venue_df_1.orderBy('ts').drop('latitude_venue','longitude_venue')\n",
    "ts_extract_df = ts_extract.toPandas()\n",
    "################### getting two hours before event, 3 hours for the duration of event, and 1 hour after event \n",
    "\n",
    "########### getting two hours before event ######################3\n",
    "df_1_hr_before_event = ts_extract_df.apply(lambda x: x[\"ts\"]-timedelta(hours=1),axis=1)\n",
    "df_1_hr_before_event_df = pd.DataFrame(df_1_hr_before_event, columns=['ts']) \n",
    "df_1_hr_before_event_df['event'] = 1 \n",
    "before_1 = pd.concat([df_1_hr_before_event_df,ts_extract_df])\n",
    "before_1['event'] = before_1['event'].astype('object')\n",
    "before_1 = before_1.sort_values(['ts','abbreviation'])\n",
    "before_1['abbreviation']= before_1['abbreviation'].bfill()\n",
    "before_1['venue_name'] = before_1['venue_name'].fillna(before_1['event'])\n",
    "before_1 = before_1.drop(columns=['event'])                                     ########### first df\n",
    "\n",
    "df_2_hrs_before_event = ts_extract_df.apply(lambda x: x[\"ts\"]-timedelta(hours=2),axis=1)\n",
    "df_2_hrs_before_event_df = pd.DataFrame(df_2_hrs_before_event, columns=['ts'])\n",
    "df_2_hrs_before_event_df['event'] = 1 \n",
    "before_2 = pd.concat([df_2_hrs_before_event_df,ts_extract_df])\n",
    "before_2['event'] = before_2['event'].astype('object')\n",
    "before_2 = before_2.sort_values(['ts','abbreviation'])\n",
    "before_2['abbreviation']= before_2['abbreviation'].bfill()\n",
    "before_2['venue_name'] = before_2['venue_name'].fillna(before_2['event'])\n",
    "before_2 = before_2.drop(columns=['event'])                                    ############ second df\n",
    "\n",
    "# ########### getting three hours during the event ######################\n",
    "df_1_hr_during_event = ts_extract_df.apply(lambda x: x[\"ts\"]+timedelta(hours=1),axis=1)\n",
    "df_1_hr_during_event_df = pd.DataFrame(df_1_hr_during_event, columns=['ts'])\n",
    "df_1_hr_during_event_df['event'] = 0 \n",
    "during_1 = pd.concat([df_1_hr_during_event_df,ts_extract_df])\n",
    "during_1['event'] = during_1['event'].astype('object')\n",
    "during_1 = during_1.sort_values(['ts','abbreviation'])\n",
    "during_1['abbreviation']= during_1['abbreviation'].ffill()\n",
    "during_1['venue_name'] = during_1['venue_name'].fillna(during_1['event'])\n",
    "during_1 = during_1.drop(columns=['event'])                                   ############ third df\n",
    "\n",
    "df_2_hr_during_event = ts_extract_df.apply(lambda x: x[\"ts\"]+timedelta(hours=2),axis=1)\n",
    "df_2_hr_during_event_df = pd.DataFrame(df_2_hr_during_event, columns=['ts'])\n",
    "df_2_hr_during_event_df['event'] = 0 \n",
    "during_2 = pd.concat([df_2_hr_during_event_df,ts_extract_df])\n",
    "during_2['event'] = during_2['event'].astype('object')\n",
    "during_2 = during_2.sort_values(['ts','abbreviation'])\n",
    "during_2['abbreviation']= during_2['abbreviation'].ffill()\n",
    "during_2['venue_name'] = during_2['venue_name'].fillna(during_2['event'])\n",
    "during_2 = during_2.drop(columns=['event'])                                  ############ 4th df\n",
    "\n",
    "df_3_hr_during_event = ts_extract_df.apply(lambda x: x[\"ts\"]+timedelta(hours=3),axis=1)\n",
    "df_3_hr_during_event_df = pd.DataFrame(df_3_hr_during_event, columns=['ts'])\n",
    "df_3_hr_during_event_df['event'] = 0 \n",
    "during_3 = pd.concat([df_3_hr_during_event_df,ts_extract_df])\n",
    "during_3['event'] = during_3['event'].astype('object')\n",
    "during_3 = during_3.sort_values(['ts','abbreviation'])\n",
    "during_3['abbreviation']= during_3['abbreviation'].ffill()\n",
    "during_3['venue_name'] = during_3['venue_name'].fillna(during_3['event'])\n",
    "during_3 = during_3.drop(columns=['event'])                                 ############ 5th df\n",
    "\n",
    "\n",
    "# ########### one hours after the event ######################\n",
    "df_1_hr_after_event = ts_extract_df.apply(lambda x: x[\"ts\"]+timedelta(hours=4),axis=1)\n",
    "df_1_hr_after_event_df = pd.DataFrame(df_1_hr_after_event, columns=['ts'])\n",
    "df_1_hr_after_event_df['event'] = 1 \n",
    "after_1 = pd.concat([df_1_hr_after_event_df,ts_extract_df])\n",
    "after_1['event'] = after_1['event'].astype('object')\n",
    "after_1 = after_1.sort_values(['ts','abbreviation'])\n",
    "after_1['abbreviation']= after_1['abbreviation'].ffill()\n",
    "after_1['venue_name'] = after_1['venue_name'].fillna(after_1['event'])\n",
    "after_1 = after_1.drop(columns=['event'])                                  ############# 6th df\n",
    "\n",
    "# combining all datasets\n",
    "full_event_date = pd.concat([before_1,before_2,during_1,during_2,during_3,after_1])\n",
    "full_event_date = full_event_date.sort_values(['ts','abbreviation'])\n",
    "full_event_date = full_event_date.drop_duplicates()\n",
    "\n",
    "d = {1.0:'1',0.0:'0'}\n",
    "full_event_date['venue_name'] = full_event_date['venue_name'].map(d)\n",
    "full_event_date['venue_name'] = full_event_date['venue_name'].fillna(1)\n",
    "full_event_date['venue_name']= full_event_date['venue_name'].astype('string')\n",
    "full_event_date['abbreviation']= full_event_date['abbreviation'].astype('string')\n",
    "\n",
    "# converting to PySpark\n",
    "full_event_df = spark.createDataFrame(full_event_date)\n",
    "full_event_df = full_event_df.withColumnRenamed('venue_name','event')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b379a903-df0d-43ea-9d30-888ed1bd20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### joining bart_venue_min_distance dataset with the df_join2 dataset\n",
    "full_event_df_1 = full_event_df.withColumnRenamed('abbreviation','origin')\n",
    "df_joined_2_2 = df_join2_1.join(full_event_df_1,['ts','origin'],'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d4a1225-2491-4f8c-b390-e1d7f2573867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/14 05:54:07 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving df_joined_2_2 to a parquet file in S3 bucket\n",
    "# df_joined_2_2.write.parquet('s3a://w210-bucket/data_wrangling/df_joined_2_2.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74945e5b-7652-41f8-b0f5-61f6a5ca0f9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Joining based on destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86ab1f49-fcf3-4cd8-9795-f8c32fe2ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading datasets\n",
    "df_joined_2_2 = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_joined_2_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b442322-63be-493b-8f72-ce14680477fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming columns \n",
    "df_joined_2_2 = df_joined_2_2.withColumnRenamed('event','event_origin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89975f4d-92a8-4161-b320-40e33f18c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joining based on destination\n",
    "full_event_df_1 = full_event_df.withColumnRenamed('abbreviation','destination')\n",
    "df_joined_3_0 = df_joined_2_2.join(full_event_df_1,['ts','destination'],'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1dc13b94-76b5-4fbc-92b4-6dfa6e0e3a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving df_joined_3_3 to a parquet file in S3 bucket\n",
    "df_joined_3_0.write.parquet('s3a://w210-bucket/data_wrangling/df_joined_3_0.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f039a12e-0d23-4a4c-9212-bd25a2fab59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_3_0 = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_joined_3_0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81b05a3c-3453-4d74-8d2b-8a9ac9a8395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- ridership_number: integer (nullable = true)\n",
      " |-- origin-des: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- quarter: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- station_origin: string (nullable = true)\n",
      " |-- latitude_wthr_origin: string (nullable = true)\n",
      " |-- longitude_wthr_origin: string (nullable = true)\n",
      " |-- wthr_station_origin: string (nullable = true)\n",
      " |-- wind_speed_origin: double (nullable = true)\n",
      " |-- air_temp_origin: double (nullable = true)\n",
      " |-- precipitation_origin: double (nullable = true)\n",
      " |-- wth_type_origin: string (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- latitude_wthr: string (nullable = true)\n",
      " |-- longitude_wthr: string (nullable = true)\n",
      " |-- wthr_station_name: string (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- air_temp: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- wth_type: string (nullable = true)\n",
      " |-- event_origin: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109242901"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined_3_0.printSchema()\n",
    "df_joined_3_0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3a42832-8bbb-41f1-944d-ae16b2082823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>destination</th>\n",
       "      <th>origin</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>ridership_number</th>\n",
       "      <th>origin-des</th>\n",
       "      <th>station_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>station</th>\n",
       "      <th>latitude_wthr</th>\n",
       "      <th>longitude_wthr</th>\n",
       "      <th>wthr_station_name</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wth_type</th>\n",
       "      <th>event_origin</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108832579</td>\n",
       "      <td>108841264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ts  destination  origin  date  hour  ridership_number  origin-des  \\\n",
       "0   0            0       0     0     0                 0           0   \n",
       "\n",
       "   station_name  longitude  latitude  ...  station  latitude_wthr  \\\n",
       "0             0          0         0  ...        0              0   \n",
       "\n",
       "   longitude_wthr  wthr_station_name  wind_speed  air_temp  precipitation  \\\n",
       "0               0                  0           0         0              0   \n",
       "\n",
       "   wth_type  event_origin      event  \n",
       "0         0     108832579  108841264  \n",
       "\n",
       "[1 rows x 32 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking null values in df_station data\n",
    "missing_counts = df_joined_3_0.select([count(when(col(c).isNull(), c)).alias(c) for c in df_joined_3_0.columns]).toPandas()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "89e6a1e4-6517-467a-917b-6cf59a0b6c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_venue_min_distance = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/bart_venue_min_distance.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "b716b14f-7321-4830-b766-67665af2a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------+---------------+------------+-----------+---------+--------------------+\n",
      "|                 ts|          venue_name|latitude_venue|longitude_venue|abbreviation|  longitude| latitude|  venue_min_distance|\n",
      "+-------------------+--------------------+--------------+---------------+------------+-----------+---------+--------------------+\n",
      "|2013-03-22 19:00:00|          Cow Palace|      37.70788|    -122.420308|        BALB|-122.433817|37.733064|0.028578434824177622|\n",
      "|2014-02-08 19:00:00|          SAP Center|     37.332240|    -121.901650|        BERY|-121.874689|37.368572| 0.04524278666262851|\n",
      "|2021-08-25 19:00:00|Oakland-Alameda C...|     37.750342|    -122.202805|        COLS|-122.196869|37.753661|0.006800871782343846|\n",
      "|2022-09-27 19:00:00|      Levi's Stadium|     37.402317|    -121.968995|        MLPT|-121.890621|37.409878| 0.07873787269797004|\n",
      "|2016-05-20 19:00:00|         Oracle Park|     37.778832|    -122.389344|        MONT|-122.401066|37.789405|0.015785867508634373|\n",
      "|2016-01-24 19:00:00|          SAP Center|     37.332240|    -121.901650|        BERY|-121.874689|37.368572| 0.04524278666262851|\n",
      "|2017-10-20 20:00:00|Event Center at SJSU|     37.335366|    -121.881039|        BERY|-121.874689|37.368572| 0.03380770527557231|\n",
      "|2018-11-23 19:00:00|Oakland-Alameda C...|     37.750342|    -122.202805|        COLS|-122.196869|37.753661|0.006800871782343846|\n",
      "|2014-03-06 19:00:00|          SAP Center|     37.332240|    -121.901650|        BERY|-121.874689|37.368572| 0.04524278666262851|\n",
      "|2016-09-06 19:00:00|Oakland-Alameda C...|     37.750342|    -122.202805|        COLS|-122.196869|37.753661|0.006800871782343846|\n",
      "|2022-06-04 13:00:00|Oakland-Alameda C...|     37.750342|    -122.202805|        COLS|-122.196869|37.753661|0.006800871782343846|\n",
      "|2016-09-18 13:00:00|         Oracle Park|     37.778832|    -122.389344|        MONT|-122.401066|37.789405|0.015785867508634373|\n",
      "|2017-05-17 12:00:00|         Oracle Park|     37.778832|    -122.389344|        MONT|-122.401066|37.789405|0.015785867508634373|\n",
      "|2018-03-18 12:00:00|Oakland-Alameda C...|     37.750342|    -122.202805|        COLS|-122.196869|37.753661|0.006800871782343846|\n",
      "|2018-08-30 19:00:00|Oakland-Alameda C...|     37.750342|    -122.202805|        COLS|-122.196869|37.753661|0.006800871782343846|\n",
      "|2018-06-01 19:00:00|         Oracle Park|     37.778832|    -122.389344|        MONT|-122.401066|37.789405|0.015785867508634373|\n",
      "|2012-11-05 19:00:00|          SAP Center|     37.332240|    -121.901650|        BERY|-121.874689|37.368572| 0.04524278666262851|\n",
      "|2018-02-09 19:00:00|Event Center at SJSU|     37.335366|    -121.881039|        BERY|-121.874689|37.368572| 0.03380770527557231|\n",
      "|2022-09-04 13:00:00|          SAP Center|     37.332240|    -121.901650|        BERY|-121.874689|37.368572| 0.04524278666262851|\n",
      "|2013-11-10 19:00:00|          SAP Center|     37.332240|    -121.901650|        BERY|-121.874689|37.368572| 0.04524278666262851|\n",
      "+-------------------+--------------------+--------------+---------------+------------+-----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bart_venue_min_distance.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "de2afeff-5531-42b4-a16a-4b5f3d01f399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------+---------------+------------+\n",
      "|                 ts|          venue_name|latitude_venue|longitude_venue|abbreviation|\n",
      "+-------------------+--------------------+--------------+---------------+------------+\n",
      "|2016-12-18 15:00:00|          SAP Center|     37.332240|    -121.901650|        BERY|\n",
      "|2019-12-20 19:00:00|        Chase Center|     37.768829|    -122.389416|        MONT|\n",
      "|2019-01-05 19:00:00|          SAP Center|     37.332240|    -121.901650|        BERY|\n",
      "|2017-05-12 20:00:00|Event Center at SJSU|     37.335366|    -121.881039|        BERY|\n",
      "|2022-06-06 19:00:00|         Oracle Park|     37.778832|    -122.389344|        MONT|\n",
      "|2012-06-23 19:00:00|          SAP Center|     37.332240|    -121.901650|        BERY|\n",
      "|2018-11-24 12:00:00|Event Center at SJSU|     37.335366|    -121.881039|        BERY|\n",
      "|2015-12-13 09:00:00|      Levi's Stadium|     37.402317|    -121.968995|        MLPT|\n",
      "|2012-07-22 19:00:00|          SAP Center|     37.332240|    -121.901650|        BERY|\n",
      "|2021-06-15 18:00:00|Oakland-Alameda C...|     37.750342|    -122.202805|        COLS|\n",
      "|2022-09-24 13:00:00|Oakland-Alameda C...|     37.750342|    -122.202805|        COLS|\n",
      "|2019-09-07 19:00:00|          SAP Center|     37.332240|    -121.901650|        BERY|\n",
      "|2022-04-29 19:00:00|         Oracle Park|     37.778832|    -122.389344|        MONT|\n",
      "|2015-10-15 19:00:00|Oakland-Alameda C...|     37.750342|    -122.202805|        COLS|\n",
      "|2019-09-21 17:00:00|Oakland-Alameda C...|     37.750342|    -122.202805|        COLS|\n",
      "|2019-05-01 19:00:00|         Oracle Park|     37.778832|    -122.389344|        MONT|\n",
      "|2016-08-27 18:00:00|         Oracle Park|     37.778832|    -122.389344|        MONT|\n",
      "|2019-05-30 19:00:00|Historic BAL Theatre|       37.7088|    -122.133087|        BAYF|\n",
      "|2021-08-18 12:00:00|         Oracle Park|     37.778832|    -122.389344|        MONT|\n",
      "|2013-03-21 12:00:00|          SAP Center|     37.332240|    -121.901650|        BERY|\n",
      "+-------------------+--------------------+--------------+---------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2621"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_filtered = sports_filtered.drop('latitude_venue','longitude_venue')\n",
    "final_venue_df = sports_filtered.join(bart_venue_min_distance, ['ts','venue_name'],'right').drop('venue_min_distance','longitude','latitude')\n",
    "final_venue_df= final_venue_df.dropDuplicates()\n",
    "final_venue_df.show()\n",
    "final_venue_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "76572d85-357c-45eb-9a5f-7bb0612b2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_filtered = sports_filtered.drop('latitude_venue','longitude_venue')\n",
    "final_venue_df = sports_filtered.join(bart_venue_min_distance, ['ts','venue_name'],'right').drop('venue_min_distance','longitude','latitude')\n",
    "final_venue_df= final_venue_df.dropDuplicates()\n",
    "final_venue_df_1 = final_venue_df.withColumn('venue_name',f.when(f.col(\"venue_name\").isNull(),f.lit(0)).otherwise(f.lit(1)))\\\n",
    "           .withColumn('venue_name',col('venue_name').cast('string'))\n",
    "\n",
    "ts_extract = final_venue_df_1.orderBy('ts').drop('latitude_venue','longitude_venue')\n",
    "ts_extract_df = ts_extract.toPandas()\n",
    "################### getting two hours before event, 3 hours for the duration of event, and 1 hour after event \n",
    "\n",
    "########### getting two hours before event ######################3\n",
    "df_1_hr_before_event = ts_extract_df.apply(lambda x: x[\"ts\"]-timedelta(hours=1),axis=1)\n",
    "df_1_hr_before_event_df = pd.DataFrame(df_1_hr_before_event, columns=['ts']) \n",
    "df_1_hr_before_event_df['event'] = 1 \n",
    "before_1 = pd.concat([df_1_hr_before_event_df,ts_extract_df])\n",
    "before_1['event'] = before_1['event'].astype('object')\n",
    "before_1 = before_1.sort_values(['ts','abbreviation'])\n",
    "before_1['abbreviation']= before_1['abbreviation'].bfill()\n",
    "before_1['venue_name'] = before_1['venue_name'].fillna(before_1['event'])\n",
    "before_1 = before_1.drop(columns=['event'])                                     ########### first df\n",
    "\n",
    "df_2_hrs_before_event = ts_extract_df.apply(lambda x: x[\"ts\"]-timedelta(hours=2),axis=1)\n",
    "df_2_hrs_before_event_df = pd.DataFrame(df_2_hrs_before_event, columns=['ts'])\n",
    "df_2_hrs_before_event_df['event'] = 1 \n",
    "before_2 = pd.concat([df_2_hrs_before_event_df,ts_extract_df])\n",
    "before_2['event'] = before_2['event'].astype('object')\n",
    "before_2 = before_2.sort_values(['ts','abbreviation'])\n",
    "before_2['abbreviation']= before_2['abbreviation'].bfill()\n",
    "before_2['venue_name'] = before_2['venue_name'].fillna(before_2['event'])\n",
    "before_2 = before_2.drop(columns=['event'])                                    ############ second df\n",
    "\n",
    "# ########### getting three hours during the event ######################\n",
    "df_1_hr_during_event = ts_extract_df.apply(lambda x: x[\"ts\"]+timedelta(hours=1),axis=1)\n",
    "df_1_hr_during_event_df = pd.DataFrame(df_1_hr_during_event, columns=['ts'])\n",
    "df_1_hr_during_event_df['event'] = 0 \n",
    "during_1 = pd.concat([df_1_hr_during_event_df,ts_extract_df])\n",
    "during_1['event'] = during_1['event'].astype('object')\n",
    "during_1 = during_1.sort_values(['ts','abbreviation'])\n",
    "during_1['abbreviation']= during_1['abbreviation'].ffill()\n",
    "during_1['venue_name'] = during_1['venue_name'].fillna(during_1['event'])\n",
    "during_1 = during_1.drop(columns=['event'])                                   ############ third df\n",
    "\n",
    "df_2_hr_during_event = ts_extract_df.apply(lambda x: x[\"ts\"]+timedelta(hours=2),axis=1)\n",
    "df_2_hr_during_event_df = pd.DataFrame(df_2_hr_during_event, columns=['ts'])\n",
    "df_2_hr_during_event_df['event'] = 0 \n",
    "during_2 = pd.concat([df_2_hr_during_event_df,ts_extract_df])\n",
    "during_2['event'] = during_2['event'].astype('object')\n",
    "during_2 = during_2.sort_values(['ts','abbreviation'])\n",
    "during_2['abbreviation']= during_2['abbreviation'].ffill()\n",
    "during_2['venue_name'] = during_2['venue_name'].fillna(during_2['event'])\n",
    "during_2 = during_2.drop(columns=['event'])                                  ############ 4th df\n",
    "\n",
    "df_3_hr_during_event = ts_extract_df.apply(lambda x: x[\"ts\"]+timedelta(hours=3),axis=1)\n",
    "df_3_hr_during_event_df = pd.DataFrame(df_3_hr_during_event, columns=['ts'])\n",
    "df_3_hr_during_event_df['event'] = 0 \n",
    "during_3 = pd.concat([df_3_hr_during_event_df,ts_extract_df])\n",
    "during_3['event'] = during_3['event'].astype('object')\n",
    "during_3 = during_3.sort_values(['ts','abbreviation'])\n",
    "during_3['abbreviation']= during_3['abbreviation'].ffill()\n",
    "during_3['venue_name'] = during_3['venue_name'].fillna(during_3['event'])\n",
    "during_3 = during_3.drop(columns=['event'])                                 ############ 5th df\n",
    "\n",
    "\n",
    "# ########### one hours after the event ######################\n",
    "df_1_hr_after_event = ts_extract_df.apply(lambda x: x[\"ts\"]+timedelta(hours=4),axis=1)\n",
    "df_1_hr_after_event_df = pd.DataFrame(df_1_hr_after_event, columns=['ts'])\n",
    "df_1_hr_after_event_df['event'] = 1 \n",
    "after_1 = pd.concat([df_1_hr_after_event_df,ts_extract_df])\n",
    "after_1['event'] = after_1['event'].astype('object')\n",
    "after_1 = after_1.sort_values(['ts','abbreviation'])\n",
    "after_1['abbreviation']= after_1['abbreviation'].ffill()\n",
    "after_1['venue_name'] = after_1['venue_name'].fillna(after_1['event'])\n",
    "after_1 = after_1.drop(columns=['event'])                                  ############# 6th df\n",
    "\n",
    "# combining all datasets\n",
    "full_event_date = pd.concat([before_1,before_2,during_1,during_2,during_3,after_1])\n",
    "full_event_date = full_event_date.sort_values(['ts','abbreviation'])\n",
    "full_event_date = full_event_date.drop_duplicates()\n",
    "\n",
    "d = {1.0:'1',0.0:'0'}\n",
    "full_event_date['venue_name'] = full_event_date['venue_name'].map(d)\n",
    "full_event_date['venue_name'] = full_event_date['venue_name'].fillna(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "c4bceba0-4eca-41d7-9110-0f992ede71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### getting two hours before event, 3 hours for the duration of event, and 1 hour after event \n",
    "\n",
    "########### getting two hours before event ######################3\n",
    "df_1_hr_before_event = ts_extract_df.apply(lambda x: x[\"ts\"]-timedelta(hours=1),axis=1)\n",
    "df_1_hr_before_event_df = pd.DataFrame(df_1_hr_before_event, columns=['ts']) \n",
    "df_1_hr_before_event_df['event'] = 1 \n",
    "before_1 = pd.concat([df_1_hr_before_event_df,ts_extract_df])\n",
    "before_1['event'] = before_1['event'].astype('object')\n",
    "before_1 = before_1.sort_values(['ts','abbreviation'])\n",
    "before_1['abbreviation']= before_1['abbreviation'].bfill()\n",
    "before_1['venue_name'] = before_1['venue_name'].fillna(before_1['event'])\n",
    "before_1 = before_1.drop(columns=['event'])                                     ########### first df\n",
    "\n",
    "df_2_hrs_before_event = ts_extract_df.apply(lambda x: x[\"ts\"]-timedelta(hours=2),axis=1)\n",
    "df_2_hrs_before_event_df = pd.DataFrame(df_2_hrs_before_event, columns=['ts'])\n",
    "df_2_hrs_before_event_df['event'] = 1 \n",
    "before_2 = pd.concat([df_2_hrs_before_event_df,ts_extract_df])\n",
    "before_2['event'] = before_2['event'].astype('object')\n",
    "before_2 = before_2.sort_values(['ts','abbreviation'])\n",
    "before_2['abbreviation']= before_2['abbreviation'].bfill()\n",
    "before_2['venue_name'] = before_2['venue_name'].fillna(before_2['event'])\n",
    "before_2 = before_2.drop(columns=['event'])                                    ############ second df\n",
    "\n",
    "# ########### getting three hours during the event ######################\n",
    "df_1_hr_during_event = ts_extract_df.apply(lambda x: x[\"ts\"]+timedelta(hours=1),axis=1)\n",
    "df_1_hr_during_event_df = pd.DataFrame(df_1_hr_during_event, columns=['ts'])\n",
    "df_1_hr_during_event_df['event'] = 0 \n",
    "during_1 = pd.concat([df_1_hr_during_event_df,ts_extract_df])\n",
    "during_1['event'] = during_1['event'].astype('object')\n",
    "during_1 = during_1.sort_values(['ts','abbreviation'])\n",
    "during_1['abbreviation']= during_1['abbreviation'].ffill()\n",
    "during_1['venue_name'] = during_1['venue_name'].fillna(during_1['event'])\n",
    "during_1 = during_1.drop(columns=['event'])                                   ############ third df\n",
    "\n",
    "df_2_hr_during_event = ts_extract_df.apply(lambda x: x[\"ts\"]+timedelta(hours=2),axis=1)\n",
    "df_2_hr_during_event_df = pd.DataFrame(df_2_hr_during_event, columns=['ts'])\n",
    "df_2_hr_during_event_df['event'] = 0 \n",
    "during_2 = pd.concat([df_2_hr_during_event_df,ts_extract_df])\n",
    "during_2['event'] = during_2['event'].astype('object')\n",
    "during_2 = during_2.sort_values(['ts','abbreviation'])\n",
    "during_2['abbreviation']= during_2['abbreviation'].ffill()\n",
    "during_2['venue_name'] = during_2['venue_name'].fillna(during_2['event'])\n",
    "during_2 = during_2.drop(columns=['event'])                                  ############ 4th df\n",
    "\n",
    "df_3_hr_during_event = ts_extract_df.apply(lambda x: x[\"ts\"]+timedelta(hours=3),axis=1)\n",
    "df_3_hr_during_event_df = pd.DataFrame(df_3_hr_during_event, columns=['ts'])\n",
    "df_3_hr_during_event_df['event'] = 0 \n",
    "during_3 = pd.concat([df_3_hr_during_event_df,ts_extract_df])\n",
    "during_3['event'] = during_3['event'].astype('object')\n",
    "during_3 = during_3.sort_values(['ts','abbreviation'])\n",
    "during_3['abbreviation']= during_3['abbreviation'].ffill()\n",
    "during_3['venue_name'] = during_3['venue_name'].fillna(during_3['event'])\n",
    "during_3 = during_3.drop(columns=['event'])                                 ############ 5th df\n",
    "\n",
    "\n",
    "# ########### one hours after the event ######################\n",
    "df_1_hr_after_event = ts_extract_df.apply(lambda x: x[\"ts\"]+timedelta(hours=4),axis=1)\n",
    "df_1_hr_after_event_df = pd.DataFrame(df_1_hr_after_event, columns=['ts'])\n",
    "df_1_hr_after_event_df['event'] = 1 \n",
    "after_1 = pd.concat([df_1_hr_after_event_df,ts_extract_df])\n",
    "after_1['event'] = after_1['event'].astype('object')\n",
    "after_1 = after_1.sort_values(['ts','abbreviation'])\n",
    "after_1['abbreviation']= after_1['abbreviation'].ffill()\n",
    "after_1['venue_name'] = after_1['venue_name'].fillna(after_1['event'])\n",
    "after_1 = after_1.drop(columns=['event'])                                  ############# 6th df\n",
    "\n",
    "# combining all datasets\n",
    "full_event_date = pd.concat([before_1,before_2,during_1,during_2,during_3,after_1])\n",
    "full_event_date = full_event_date.sort_values(['ts','abbreviation'])\n",
    "full_event_date = full_event_date.drop_duplicates()\n",
    "\n",
    "d = {1.0:'1',0.0:'0'}\n",
    "full_event_date['venue_name'] = full_event_date['venue_name'].map(d)\n",
    "full_event_date['venue_name'] = full_event_date['venue_name'].fillna(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cb6d60dc-3c94-4a31-8c88-71cee8c8f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################### getting two hours before event, 3 hours for the duration of event, and 1 hour after event \n",
    "\n",
    "# ########### getting two hours before event ######################3\n",
    "# df_1_hr_before_event = ts_extract_df.apply(lambda x: x[\"ts\"]-timedelta(hours=1),axis=1)\n",
    "# df_1_hr_before_event_df = pd.DataFrame(df_1_hr_before_event, columns=['ts']) \n",
    "# df_1_hr_before_event_df['event'] = 1 # first df\n",
    "\n",
    "# df_2_hrs_before_event = df_1_hr_before_event_df.apply(lambda x: x[\"ts\"]-timedelta(hours=1),axis=1)\n",
    "# df_2_hrs_before_event_df = pd.DataFrame(df_2_hrs_before_event, columns=['ts'])\n",
    "# df_2_hrs_before_event_df['event'] = 1 # second df\n",
    "\n",
    "\n",
    "# ########### getting three hours during the event ######################\n",
    "# df_1_hr_after_event = ts_extract_df.apply(lambda x: x[\"ts\"]+timedelta(hours=1),axis=1)\n",
    "# df_1_hr_after_event_df = pd.DataFrame(df_1_hr_after_event, columns=['ts'])\n",
    "# df_1_hr_after_event_df['event'] = 0 # third df\n",
    "\n",
    "# df_2_hr_after_event = df_1_hr_after_event_df.apply(lambda x: x[\"ts\"]+timedelta(hours=1),axis=1)\n",
    "# df_2_hr_after_event_df = pd.DataFrame(df_2_hr_after_event, columns=['ts'])\n",
    "# df_2_hr_after_event_df['event'] = 0 # fourth df\n",
    "\n",
    "# df_3_hr_after_event = df_2_hr_after_event_df.apply(lambda x: x[\"ts\"]+timedelta(hours=1),axis=1)\n",
    "# df_3_hr_after_event_df = pd.DataFrame(df_3_hr_after_event, columns=['ts'])\n",
    "# df_3_hr_after_event_df['event'] = 0 # fifth df\n",
    "\n",
    "# ########### one hours after the event ######################\n",
    "# df_1_hr_event_end = df_3_hr_after_event_df.apply(lambda x: x[\"ts\"]+timedelta(hours=1),axis=1)\n",
    "# df_1_hr_event_end_df = pd.DataFrame(df_1_hr_event_end, columns=['ts'])\n",
    "# df_1_hr_event_end_df['event']= 1 # sixth df\n",
    "\n",
    "# full_event_date = pd.concat([ts_extract_df,df_1_hr_before_event_df,df_2_hrs_before_event_df,df_1_hr_after_event_df,\n",
    "#                              df_2_hr_after_event_df,df_3_hr_after_event_df,df_1_hr_event_end_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7569d810-e780-45c1-aa08-29d2c6b360e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_event_date =full_event_date.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bce83b9c-1438-49d6-acee-0d46b4399393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-12-15 17:00:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-12-15 18:00:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-12-15 19:00:00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-12-15 20:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-12-15 21:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ts  event\n",
       "0 2011-12-15 17:00:00    1.0\n",
       "0 2011-12-15 18:00:00    1.0\n",
       "0 2011-12-15 19:00:00    1.0\n",
       "0 2011-12-15 20:00:00    0.0\n",
       "0 2011-12-15 21:00:00    0.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_event_date.sort_values('ts').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e27626f9-8358-4f6c-90cb-f11053c2b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_venue_df_pandas = final_venue_df_1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9178da21-b81d-4e13-a57c-92ce2a398b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>venue_name</th>\n",
       "      <th>latitude_venue</th>\n",
       "      <th>longitude_venue</th>\n",
       "      <th>abbreviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-18 15:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>37.332240</td>\n",
       "      <td>-121.901650</td>\n",
       "      <td>BERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-12-20 19:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>37.768829</td>\n",
       "      <td>-122.389416</td>\n",
       "      <td>MONT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-05 19:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>37.332240</td>\n",
       "      <td>-121.901650</td>\n",
       "      <td>BERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-12 20:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>37.335366</td>\n",
       "      <td>-121.881039</td>\n",
       "      <td>BERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-06-06 19:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>37.778832</td>\n",
       "      <td>-122.389344</td>\n",
       "      <td>MONT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>2020-03-04 20:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>37.750342</td>\n",
       "      <td>-122.202805</td>\n",
       "      <td>COLS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>2013-03-14 19:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>37.332240</td>\n",
       "      <td>-121.901650</td>\n",
       "      <td>BERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>2012-11-10 19:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>37.70788</td>\n",
       "      <td>-122.420308</td>\n",
       "      <td>BALB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>2016-02-29 19:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>37.332240</td>\n",
       "      <td>-121.901650</td>\n",
       "      <td>BERY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2939</th>\n",
       "      <td>2017-08-05 16:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>37.766948</td>\n",
       "      <td>-122.456000</td>\n",
       "      <td>16TH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2940 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ts venue_name latitude_venue longitude_venue  \\\n",
       "0    2016-12-18 15:00:00          1      37.332240     -121.901650   \n",
       "1    2019-12-20 19:00:00          1      37.768829     -122.389416   \n",
       "2    2019-01-05 19:00:00          1      37.332240     -121.901650   \n",
       "3    2017-05-12 20:00:00          1      37.335366     -121.881039   \n",
       "4    2022-06-06 19:00:00          1      37.778832     -122.389344   \n",
       "...                  ...        ...            ...             ...   \n",
       "2935 2020-03-04 20:00:00          1      37.750342     -122.202805   \n",
       "2936 2013-03-14 19:00:00          1      37.332240     -121.901650   \n",
       "2937 2012-11-10 19:00:00          1       37.70788     -122.420308   \n",
       "2938 2016-02-29 19:00:00          1      37.332240     -121.901650   \n",
       "2939 2017-08-05 16:00:00          1      37.766948     -122.456000   \n",
       "\n",
       "     abbreviation  \n",
       "0            BERY  \n",
       "1            MONT  \n",
       "2            BERY  \n",
       "3            BERY  \n",
       "4            MONT  \n",
       "...           ...  \n",
       "2935         COLS  \n",
       "2936         BERY  \n",
       "2937         BALB  \n",
       "2938         BERY  \n",
       "2939         16TH  \n",
       "\n",
       "[2940 rows x 5 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venue_final_df  = pd.merge(full_event_date,final_venue_df_pandas, on = 'ts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1002b067-a68e-4cb6-bde2-2d1725328f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2412"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_venue_df_pandas['ts'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0e007f0e-381b-40d7-9e1b-42e93da84f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = final_venue_df_pandas.drop_duplicates(subset=[\"ts\"], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2695ef71-973f-4aaa-9431-214c9a28f117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1939"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sss['ts'].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
