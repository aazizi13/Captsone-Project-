{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3c3cd2-c6e1-43e9-8f16-ff0499650d58",
   "metadata": {},
   "source": [
    "# Data Wrangling Notebook 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8bdfa2-819d-4ba4-80fd-7f704b660808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import split, slice, count, when, expr, isnan, isnull\n",
    "from pyspark.sql.functions import date_format, to_timestamp, concat, unix_timestamp, substring, lit\n",
    "from pyspark.sql.functions import col, month, quarter, dayofweek, year\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "from pyspark.sql.window import Window\n",
    "import configparser\n",
    "import findspark\n",
    "import lxml\n",
    "from datetime import timedelta\n",
    "from pandas.tseries.offsets import BDay\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa25a4-0309-4362-a308-2b0d1d50e207",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reading From S3 Bucket using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db130b-4b36-4d96-87f5-f0eea6ed9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# This code block is used for pulling data from S3 using boto3 library\n",
    "############# Note: Not necessary for this project\n",
    "\n",
    "#getting relevant information for the S3 bucket\n",
    "\n",
    "# AWS_S3_BUCKET='w210-bucket'\n",
    "# AWS_S3_REGION='us-east-1'\n",
    "# AWS_PROFILE_NAME='default'\n",
    "# session = boto3.Session(profile_name=AWS_PROFILE_NAME)\n",
    "# s3 = session.resource('s3')\n",
    "# s3_client = session.client('s3',region_name=AWS_S3_REGION)\n",
    "# my_bucket = s3.Bucket(AWS_S3_BUCKET)\n",
    "\n",
    "# #printing all the files in ridership directory \n",
    "# for objects in my_bucket.objects.filter(Prefix=\"ridership/\"):\n",
    "#     print(objects.key)\n",
    "\n",
    "# #printing all the files in weather directory \n",
    "# for objects in my_bucket.objects.filter(Prefix=\"weather/\"):\n",
    "#     print(objects.key)\n",
    "\n",
    "# #reading one file only as pandas df\n",
    "# obj = s3_client.get_object(Bucket= AWS_S3_BUCKET, Key= \"ridership/date-hour-soo-dest-2011.csv\") \n",
    "# # get object and file (key) from bucket\n",
    "# initial_df = pd.read_csv(obj['Body'], header=None) \n",
    "# initial_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef685327-0b87-4a4e-a1b1-bd046c494607",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reading From S3 Bucket using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35af90ae-44cd-4deb-bcc6-83c480a410cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starting Pyspark Session\n",
    "\n",
    "# spark = SparkSession.builder\\\n",
    "#                     .config('spark.master','local[*]')\\\n",
    "#                     .config('spark.add.name','S3app')\\\n",
    "#                     .config('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-common:3.3.4')\\\n",
    "#                     .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .config('spark.master','local[*]')\\\n",
    "                    .config('spark.add.name','S3app')\\\n",
    "                    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "                    .config('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-common:3.3.4')\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8364aa13-912e-4382-a45c-1e2856ee137c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-13-189.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcb10fe6130>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49bf5dc4-14c5-4b00-924b-28791908fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configuring Pyspark to read data from S3 Bucket. \n",
    "\n",
    "findspark.init()\n",
    "config = configparser.ConfigParser()\n",
    "# AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "# AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "aws_profile = 'default'\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "access_id = config.get(aws_profile, \"aws_access_key_id\") \n",
    "access_key = config.get(aws_profile, \"aws_secret_access_key\")\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)\n",
    "# spark.conf.set(\"spark.sql.broadcastTimeout\", 3000)\n",
    "\n",
    "# spark.conf.set(\"spark.driver.maxResultSize\",8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.committer.name\",\"magic\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.awsAccessKeyId\", access_id)\n",
    "hadoop_conf.set(\"fs.s3a.awsSecretAccessKey\", access_key)\n",
    "hadoop_conf.set('spark.sql.files.maxPartitionBytes','134217728')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "29843d0f-4c94-4606-a44d-76ca3cead52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa361767-9a08-4ec4-9f0a-119bed90acdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ridership Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549a440-2661-4ac8-b145-568a6ec34447",
   "metadata": {},
   "source": [
    "### In this section of Notebook, we will read all the ridership data and do data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "699e4985-af69-4191-bc39-35def970a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading all the ridership data\n",
    "ridership_2011 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2011.csv\")\n",
    "ridership_2012 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2012.csv\")\n",
    "ridership_2013 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2013.csv\")\n",
    "ridership_2014 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2014.csv\")\n",
    "ridership_2015 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2015.csv\")\n",
    "ridership_2016 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2016.csv\")\n",
    "ridership_2017 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2017.csv\")\n",
    "ridership_2018 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2018.csv\")\n",
    "ridership_2019 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2019.csv\")\n",
    "ridership_2020 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2020.csv\")\n",
    "ridership_2021 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2021.csv\")\n",
    "ridership_2022 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e1fa0900-620e-4031-aaa3-808b059f8c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      " |-- ridership_number: integer (nullable = true)\n",
      " |-- origin-des: string (nullable = true)\n",
      "\n",
      "+----------+----+------+-----------+----------------+----------+\n",
      "|      date|hour|origin|destination|ridership_number|origin-des|\n",
      "+----------+----+------+-----------+----------------+----------+\n",
      "|2011-01-01|   0|  12TH|       12TH|               1| 12TH-12TH|\n",
      "|2011-01-01|   0|  12TH|       16TH|               1| 12TH-16TH|\n",
      "|2011-01-01|   0|  12TH|       24TH|               3| 12TH-24TH|\n",
      "|2011-01-01|   0|  12TH|       ASHB|               2| 12TH-ASHB|\n",
      "|2011-01-01|   0|  12TH|       BAYF|               5| 12TH-BAYF|\n",
      "|2011-01-01|   0|  12TH|       CIVC|               3| 12TH-CIVC|\n",
      "|2011-01-01|   0|  12TH|       COLS|               1| 12TH-COLS|\n",
      "|2011-01-01|   0|  12TH|       CONC|               2| 12TH-CONC|\n",
      "|2011-01-01|   0|  12TH|       DALY|               1| 12TH-DALY|\n",
      "|2011-01-01|   0|  12TH|       DBRK|               4| 12TH-DBRK|\n",
      "+----------+----+------+-----------+----------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all the ridership data from 2011 to 2022\n",
    "df_ridership_all = ridership_2011.union(ridership_2012).union(ridership_2013).union(ridership_2014)\\\n",
    "                             .union(ridership_2015).union(ridership_2016).union(ridership_2017)\\\n",
    "                             .union(ridership_2018).union(ridership_2019).union(ridership_2020)\\\n",
    "                             .union(ridership_2021).union(ridership_2022)\n",
    "\n",
    "#Rename columns and creating a new column called origin-destination pair \n",
    "df_ridership_all =df_ridership_all.withColumnRenamed('_c0',\"date\") \\\n",
    "        .withColumnRenamed('_c1','hour') \\\n",
    "        .withColumnRenamed('_c2','origin') \\\n",
    "        .withColumnRenamed('_c3','destination') \\\n",
    "        .withColumnRenamed('_c4','ridership_number') \\\n",
    "        .withColumn('origin-des', concat(col('origin'), lit('-'), col('destination')))\n",
    "\n",
    "# change data schema\n",
    "df_ridership_all = df_ridership_all.withColumn(\"date\",col(\"date\").cast(DateType())) \\\n",
    "    .withColumn(\"hour\",col(\"hour\").cast(IntegerType())) \\\n",
    "    .withColumn(\"ridership_number\",col(\"ridership_number\").cast(IntegerType()))\n",
    "\n",
    "# print the schema and the first 5 rows\n",
    "df_ridership_all.printSchema()\n",
    "df_ridership_all.show(10)\n",
    "# df_ridership_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b9c5810d-ca0b-45e6-8066-f64644451900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abbreviation: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      "\n",
      "+------------+--------------------+-----------+---------+\n",
      "|abbreviation|        station_name|  longitude| latitude|\n",
      "+------------+--------------------+-----------+---------+\n",
      "|        12TH|12th St. Oakland ...|-122.271450|37.803768|\n",
      "|        16TH|16th St. Mission ...|-122.419694|37.765062|\n",
      "|        19TH|19th St. Oakland ...|-122.268602|37.808350|\n",
      "|        24TH|24th St. Mission ...|-122.418143|37.752470|\n",
      "|        ASHB|        Ashby (ASHB)|-122.270062|37.852803|\n",
      "|        BALB|  Balboa Park (BALB)|-122.447506|37.721585|\n",
      "|        BAYF|     Bay Fair (BAYF)|-122.126514|37.696924|\n",
      "|        CAST|Castro Valley (CAST)|-122.075602|37.690746|\n",
      "|        CIVC|Civic Center/UN P...|-122.414123|37.779732|\n",
      "|        COLS|Coliseum/Oakland ...|-122.196869|37.753661|\n",
      "+------------+--------------------+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### read the station data to get station abbreviations, Longitude, and Latitude\n",
    "\n",
    "df_station = spark.read.option(\"header\",True) \\\n",
    "           .csv(\"s3a://w210-bucket/ridership/station_bart.csv\")\n",
    "\n",
    "#rename columns\n",
    "df_station = df_station.withColumnRenamed('Abbreviation','abbreviation') \\\n",
    "                        .withColumnRenamed('Location','location')\\\n",
    "                        .withColumnRenamed('Name','station_name').drop('description').drop('name')\n",
    "\n",
    "#split longitude and latitude into separate columns\n",
    "df_station = df_station.withColumn(\"longitude\", split(col(\"location\"), \",\").getItem(0)) \\\n",
    "                         .withColumn(\"latitude\", split(col(\"location\"), \",\").getItem(1)).drop('location')\n",
    "\n",
    "# print schema and the first 5 rows\n",
    "df_station.printSchema()\n",
    "df_station.show(10)\n",
    "# df_station.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "29b8f5aa-eb42-4563-adb5-c71ac5e33495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>station_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   abbreviation  station_name  longitude  latitude\n",
       "0             0             0          0         0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking null values in df_station data\n",
    "missing_counts = df_station.select([count(when(col(c).isNull(), c)).alias(c) for c in df_station.columns]).toPandas()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce4fa5a6-17c8-48b7-bc06-42fa0e9013ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique number of bart stations in df_station: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:======================================================> (94 + 2) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique number of bart stations in df_ridership_all: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# looking at unique bart stations in df_station and ridership data\n",
    "print(f\"Unique number of bart stations in df_station: {df_station.select('abbreviation').distinct().count()}\")\n",
    "print(f\"Unique number of bart stations in df_ridership_all: {df_ridership_all.select('origin').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3a889153-0210-4b6a-b3db-3010f33e2b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# finding out which bart stations don't exists\n",
    "stations_df_stations = df_station.select('abbreviation').distinct().toPandas()\n",
    "stations_df_ridership_all = df_ridership_all.select('origin').distinct().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "16a6417f-8c78-4723-9f09-ac3a708623fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ANTC', 'PCTR', 'BERY', 'MLPT']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df_stations_list = stations_df_stations['abbreviation'].tolist()\n",
    "stations_df_ridership_all_list = stations_df_ridership_all['origin'].tolist()\n",
    "\n",
    "missing_bart_stations = [x for x in stations_df_ridership_all_list if x not in  stations_df_stations_list]\n",
    "missing_bart_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1d12e140-d89b-451c-b41a-30a6b5862de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the missing station data to the df_station data\n",
    "columns = ['abbreviation', 'station_name', 'longitude', 'latitude']\n",
    "data = [\\\n",
    "        ('ANTC','Antioch Station', '-121.780320', '37.996012'),\\\n",
    "        ('PCTR','Pittsburg Center Station', '-121.888538', '38.018200'),\\\n",
    "        ('BERY','Berryessa Bart Station', '-121.874689', '37.368572'),\\\n",
    "        ('MLPT','Milpitas Bart Station', '-121.890621', '37.409878')]\n",
    "\n",
    "missing_stations = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_station_full = df_station.union(missing_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4d624da9-16ce-481e-899a-07c0dfdcc00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      " |-- ridership_number: integer (nullable = true)\n",
      " |-- origin-des: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- quarter: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 344:===========>   (76 + 8) / 96][Stage 345:>                (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+-----------+----------------+----------+--------------------+-----------+---------+----+-----+-------+-----------+-------------------+\n",
      "|      date|hour|origin|destination|ridership_number|origin-des|        station_name|  longitude| latitude|year|month|quarter|day_of_week|                 ts|\n",
      "+----------+----+------+-----------+----------------+----------+--------------------+-----------+---------+----+-----+-------+-----------+-------------------+\n",
      "|2011-01-01|   0|  12TH|       12TH|               1| 12TH-12TH|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       16TH|               1| 12TH-16TH|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       24TH|               3| 12TH-24TH|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       ASHB|               2| 12TH-ASHB|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       BAYF|               5| 12TH-BAYF|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       CIVC|               3| 12TH-CIVC|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       COLS|               1| 12TH-COLS|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       CONC|               2| 12TH-CONC|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       DALY|               1| 12TH-DALY|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       DBRK|               4| 12TH-DBRK|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "+----------+----+------+-----------+----------------+----------+--------------------+-----------+---------+----+-----+-------+-----------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### join ridership data to station data based on origin\n",
    "\n",
    "df_station_ridership = df_ridership_all.join(df_station_full,df_ridership_all.origin ==  df_station_full.abbreviation,\"left\").drop('abbreviation')\n",
    "\n",
    "#extract month, quarter, and day_of_week from the date column\n",
    "df_station_ridership = df_station_ridership.withColumn('year', year(df_station_ridership.date)) \\\n",
    "                                            .withColumn('month', month(df_station_ridership.date)) \\\n",
    "                                            .withColumn('quarter', quarter(df_station_ridership.date)) \\\n",
    "                                            .withColumn('day_of_week', dayofweek(df_station_ridership.date)) \n",
    "\n",
    "#set timeparser policy to legacy \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "#create timestamp\n",
    "df_station_ridership = df_station_ridership.withColumn(\"ts\",f.to_timestamp(f.concat(\"date\",\"hour\"),\"yyyy-MM-ddHH\"))\n",
    "\n",
    "#change data schema and print first 5 rows\n",
    "df_station_ridership = df_station_ridership.withColumn(\"date\",col(\"date\").cast(DateType())) \\\n",
    "    .withColumn(\"hour\",col(\"hour\").cast('string')) \\\n",
    "    .withColumn(\"year\",col(\"year\").cast('string')) \\\n",
    "    .withColumn(\"month\",col(\"month\").cast('string')) \\\n",
    "    .withColumn(\"quarter\",col(\"quarter\").cast('string')) \\\n",
    "    .withColumn(\"day_of_week\",col(\"day_of_week\").cast('string'))\n",
    "    # .withColumn('ts_long', col('ts').cast('long'))\n",
    "\n",
    "df_station_ridership.printSchema()\n",
    "df_station_ridership.show(10)\n",
    "# df_station_ridership.count()\n",
    "# df_station_ridership.select('ts_long').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "05639b37-4f93-479d-b892-ae26ba9bacb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>ridership_number</th>\n",
       "      <th>origin-des</th>\n",
       "      <th>station_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date  hour  origin  destination  ridership_number  origin-des  \\\n",
       "0     0     0       0            0                 0           0   \n",
       "\n",
       "   station_name  longitude  latitude  year  month  quarter  day_of_week  ts  \n",
       "0             0          0         0     0      0        0            0   0  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for nulls in df_station_ridership data\n",
    "missing_counts = df_station_ridership.select([count(when(col(c).isNull(), c)).alias(c) for c in df_station_ridership.columns]).toPandas()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f8922-5493-4f63-8909-2f8d8c10df34",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205278b0-91cf-40a9-aaf0-e6dbb8d1cf2b",
   "metadata": {},
   "source": [
    "### In this section, we will read all the weather data and do some data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e56b9752-59bc-49fa-a224-849d30cacd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custome Functions\n",
    "\n",
    "def weather_read(df):\n",
    "    \"\"\"\n",
    "    Input = file path in S3 Bucket\n",
    "    Ouput = PySpark DataFrame\n",
    "    \"\"\"\n",
    "    path = df\n",
    "    df = spark.read.option('header',True).csv(path)\n",
    "    return df\n",
    "\n",
    "def weather_cleanup(df):\n",
    "    \"\"\"\n",
    "    Input: Weather station PySpark DataFrame\n",
    "    Operation: selects columns, gets rid of minutes and zeroes out the hours,\n",
    "               creates timestamp, and drops duplicate rows for timestamps\n",
    "               \n",
    "    Output: Pyspark DataFrame\n",
    "    \"\"\"\n",
    "    df = df.withColumn('date1',concat(substring('DATE',1,10), lit(' '), substring('DATE', 12,2), lit(':00:00'))) \\\n",
    "           .withColumn('ts', f.to_timestamp('date1',\"yyyy-MM-dd HH:mm:ss\")).drop('date1').dropDuplicates(['ts'])\\\n",
    "           .select('ts','STATION','DATE','SOURCE','LATITUDE','LONGITUDE','ELEVATION','NAME',\n",
    "                   'REPORT_TYPE','CALL_SIGN','QUALITY_CONTROL','WND','CIG','VIS','TMP','DEW','SLP','AA1','AT1','AU1')\n",
    "           # .withColumn('ts_long', col('ts').cast('long'))\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a2e2ba4f-616e-44ce-9c24-ac9200d50843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# San Franciso station\n",
    "sf_2011 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2011.csv\")\n",
    "sf_2011 = weather_cleanup(sf_2011)\n",
    "sf_2012 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2012.csv\")\n",
    "sf_2012 = weather_cleanup(sf_2012)\n",
    "sf_2013 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2013.csv\")\n",
    "sf_2013 = weather_cleanup(sf_2013)\n",
    "sf_2014 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2014.csv\")\n",
    "sf_2014 = weather_cleanup(sf_2014)\n",
    "sf_2015 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2015.csv\")\n",
    "sf_2015 = weather_cleanup(sf_2015)\n",
    "sf_2016 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2016.csv\")\n",
    "sf_2016 = weather_cleanup(sf_2016)\n",
    "sf_2017 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2017.csv\")\n",
    "sf_2017 = weather_cleanup(sf_2017)\n",
    "sf_2018 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2018.csv\")\n",
    "sf_2018 = weather_cleanup(sf_2018)\n",
    "sf_2019 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2019.csv\")\n",
    "sf_2019 = weather_cleanup(sf_2019)\n",
    "sf_2020 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2020.csv\")\n",
    "sf_2020 = weather_cleanup(sf_2020)\n",
    "sf_2021 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2021.csv\")\n",
    "sf_2021 = weather_cleanup(sf_2021)\n",
    "sf_2022 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2022.csv\")\n",
    "sf_2022 = weather_cleanup(sf_2022)\n",
    "\n",
    "# Concord Station\n",
    "concord_2011 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2011.csv\")\n",
    "concord_2011 = weather_cleanup(concord_2011)\n",
    "concord_2012 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2012.csv\")\n",
    "concord_2012 = weather_cleanup(concord_2012)\n",
    "concord_2013 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2013.csv\")\n",
    "concord_2013 = weather_cleanup(concord_2013)\n",
    "concord_2014 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2014.csv\")\n",
    "concord_2014 = weather_cleanup(concord_2014)\n",
    "concord_2015 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2015.csv\")\n",
    "concord_2015 = weather_cleanup(concord_2015)\n",
    "concord_2016 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2016.csv\")\n",
    "concord_2016 = weather_cleanup(concord_2016)\n",
    "concord_2017 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2017.csv\")\n",
    "concord_2017 = weather_cleanup(concord_2017)\n",
    "concord_2018 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2018.csv\")\n",
    "concord_2018 = weather_cleanup(concord_2018)\n",
    "concord_2019 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2019.csv\")\n",
    "concord_2019 = weather_cleanup(concord_2019)\n",
    "concord_2020 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2020.csv\")\n",
    "concord_2020 = weather_cleanup(concord_2020)\n",
    "concord_2021 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2021.csv\")\n",
    "concord_2021 = weather_cleanup(concord_2021)\n",
    "concord_2022 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2022.csv\")\n",
    "concord_2022 = weather_cleanup(concord_2022)\n",
    "\n",
    "\n",
    "# livermore Station\n",
    "livermore_2011 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2011.csv\")\n",
    "livermore_2011 = weather_cleanup(livermore_2011)\n",
    "livermore_2012 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2012.csv\")\n",
    "livermore_2012 = weather_cleanup(livermore_2012)\n",
    "livermore_2013 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2013.csv\")\n",
    "livermore_2013 = weather_cleanup(livermore_2013)\n",
    "livermore_2014 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2014.csv\")\n",
    "livermore_2014 = weather_cleanup(livermore_2014)\n",
    "livermore_2015 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2015.csv\")\n",
    "livermore_2015 = weather_cleanup(livermore_2015)\n",
    "livermore_2016 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2016.csv\")\n",
    "livermore_2016 = weather_cleanup(livermore_2016)\n",
    "livermore_2017 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2017.csv\")\n",
    "livermore_2017 = weather_cleanup(livermore_2017)\n",
    "livermore_2018 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2018.csv\")\n",
    "livermore_2018 = weather_cleanup(livermore_2018)\n",
    "livermore_2019 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2019.csv\")\n",
    "livermore_2019 = weather_cleanup(livermore_2019)\n",
    "livermore_2020 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2020.csv\")\n",
    "livermore_2020 = weather_cleanup(livermore_2020)\n",
    "livermore_2021 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2021.csv\")\n",
    "livermore_2021 = weather_cleanup(livermore_2021)\n",
    "livermore_2022 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2022.csv\")\n",
    "livermore_2022 = weather_cleanup(livermore_2022)\n",
    "\n",
    "\n",
    "# San Jose Station\n",
    "sanjose_2011 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2011.csv\")\n",
    "sanjose_2011 = weather_cleanup(sanjose_2011)\n",
    "sanjose_2012 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2012.csv\")\n",
    "sanjose_2012 = weather_cleanup(sanjose_2012)\n",
    "sanjose_2013 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2013.csv\")\n",
    "sanjose_2013 = weather_cleanup(sanjose_2013)\n",
    "sanjose_2014 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2014.csv\")\n",
    "sanjose_2014 = weather_cleanup(sanjose_2014)\n",
    "sanjose_2015 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2015.csv\")\n",
    "sanjose_2015 = weather_cleanup(sanjose_2015)\n",
    "sanjose_2016 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2016.csv\")\n",
    "sanjose_2016 = weather_cleanup(sanjose_2016)\n",
    "sanjose_2017 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2017.csv\")\n",
    "sanjose_2017 = weather_cleanup(sanjose_2017)\n",
    "sanjose_2018 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2018.csv\")\n",
    "sanjose_2018 = weather_cleanup(sanjose_2018)\n",
    "sanjose_2019 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2019.csv\")\n",
    "sanjose_2019 = weather_cleanup(sanjose_2019)\n",
    "sanjose_2020 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2020.csv\")\n",
    "sanjose_2020 = weather_cleanup(sanjose_2020)\n",
    "sanjose_2021 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2021.csv\")\n",
    "sanjose_2021 = weather_cleanup(sanjose_2021)\n",
    "sanjose_2022 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2022.csv\")\n",
    "sanjose_2022 = weather_cleanup(sanjose_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d51e1409-d7d3-4345-a008-568863773077",
   "metadata": {},
   "outputs": [],
   "source": [
    "### concatenate weather station data per year\n",
    "\n",
    "# San Francisco\n",
    "sf = sf_2011.union(sf_2012).union(sf_2013).union(sf_2014).union(sf_2015)\\\n",
    "            .union(sf_2016).union(sf_2017).union(sf_2018).union(sf_2019)\\\n",
    "            .union(sf_2020).union(sf_2021).union(sf_2022)\n",
    "\n",
    "# Concord\n",
    "concord = concord_2011.union(concord_2012).union(concord_2013).union(concord_2014).union(concord_2015)\\\n",
    "            .union(concord_2016).union(concord_2017).union(concord_2018).union(concord_2019)\\\n",
    "            .union(concord_2020).union(concord_2021).union(concord_2022)\n",
    "        \n",
    "# Livermore\n",
    "livemore = livermore_2011.union(livermore_2012).union(livermore_2013).union(livermore_2014).union(livermore_2015)\\\n",
    "            .union(livermore_2016).union(livermore_2017).union(livermore_2018).union(livermore_2019)\\\n",
    "            .union(livermore_2020).union(livermore_2021).union(livermore_2022)\n",
    "        \n",
    "# San Jose\n",
    "sanjose = sanjose_2011.union(sanjose_2012).union(sanjose_2013).union(sanjose_2014).union(sanjose_2015)\\\n",
    "            .union(sanjose_2016).union(sanjose_2017).union(sanjose_2018).union(sanjose_2019)\\\n",
    "            .union(sanjose_2020).union(sanjose_2021).union(sanjose_2022)\n",
    "        \n",
    "### concatenate all weather stations \n",
    "weather = sf.union(concord).union(livemore).union(sanjose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ce4c333-7a26-495d-851c-645d8d653d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'San Francisco Count: {sf.count()}')\n",
    "# print(f'Concord Count: {concord.count()}')\n",
    "# print(f'Livermore Count: {livemore.count()}')\n",
    "# print(f'San Jose Count: {sanjose.count()}')\n",
    "# print(f'Total Count: {weather.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad0d2564-81aa-4ce0-8981-2cb88aa64411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- LATITUDE: string (nullable = true)\n",
      " |-- LONGITUDE: string (nullable = true)\n",
      " |-- ELEVATION: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- REPORT_TYPE: string (nullable = true)\n",
      " |-- CALL_SIGN: string (nullable = true)\n",
      " |-- QUALITY_CONTROL: string (nullable = true)\n",
      " |-- WND: string (nullable = true)\n",
      " |-- CIG: string (nullable = true)\n",
      " |-- VIS: string (nullable = true)\n",
      " |-- TMP: string (nullable = true)\n",
      " |-- DEW: string (nullable = true)\n",
      " |-- SLP: string (nullable = true)\n",
      " |-- AA1: string (nullable = true)\n",
      " |-- AT1: string (nullable = true)\n",
      " |-- AU1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3c105ad6-9620-4861-ad24-c02f981fae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 224:>  (0 + 2) / 2][Stage 225:>  (0 + 2) / 2][Stage 226:>  (0 + 2) / 2]2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 04:19:50 WARN DAGScheduler: Broadcasting large task binary with size 1345.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# extracting needed columns from existing columns \n",
    "weather2 = weather.withColumn('wind_speed',f.split(weather['WND'], ',').getItem(3).cast('integer')) \\\n",
    "                  .withColumn('air_temp',f.split(weather['TMP'], ',').getItem(0).cast('integer')) \\\n",
    "                  .withColumn('precipitation',f.split(weather['AA1'], ',').getItem(0).cast('integer')) \\\n",
    "                  .withColumn('wth_type',f.split(weather['AU1'], ',').getItem(2).cast('string'))\n",
    "\n",
    "#putting null for values of 999, 99 and so on based on the documentation\n",
    "# dropping unnessary columns \n",
    "\n",
    "weather2 = weather2.withColumn('air_temp', f.when(weather2['air_temp'] == 9999, None).otherwise(weather2['air_temp'])) \\\n",
    "                   .withColumn('wind_speed', f.when(weather2['wind_speed'] == 9999, None).otherwise(weather2['wind_speed']))\\\n",
    "                   .withColumn('precipitation', f.when(weather2['precipitation'] == 99, None).otherwise(weather2['precipitation'])) \\\n",
    "                   .drop('REPORT_TYPE','CALL_SIGN','QUALITY_CONTROL','ELEVATION','SOURCE','WND',\n",
    "                         'CIG','VIS','TMP','DEW','SLP','AA1','AT1','AU1','DATE')\n",
    "\n",
    "# converting PySpark dataframe to Pandas Dataframe\n",
    "weather3 = weather2.toPandas()\n",
    "\n",
    "# forward filling for missing values \n",
    "for col in ['wind_speed','air_temp','precipitation']:\n",
    "    weather3[col] = weather3[col].ffill()\n",
    "\n",
    "weather3['precipitation'] = weather3['precipitation'].bfill()\n",
    "    \n",
    "# mapping weather types to values \n",
    "d = {'00': 'no_precipitation', '01': 'Drizzle', '02': 'Rain',\n",
    "     '03': 'Snow', '08': 'hail', '09': None}\n",
    "\n",
    "weather3['wth_type'] = weather3['wth_type'].map(d).ffill()\n",
    "weather3['wth_type'] = weather3['wth_type'].bfill()\n",
    "\n",
    "#converting back to PysSpark Dataframe\n",
    "weather4 = spark.createDataFrame(weather3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04352d82-50c5-48c6-8d83-6dec57fa0dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 03:33:26 WARN TaskSetManager: Stage 435 contains a task of very large size (6272 KiB). The maximum recommended task size is 1000 KiB.\n",
      "+-------------------+-----------+--------+---------+--------------------+----------+--------+-------------+--------+\n",
      "|                 ts|    STATION|LATITUDE|LONGITUDE|                NAME|wind_speed|air_temp|precipitation|wth_type|\n",
      "+-------------------+-----------+--------+---------+--------------------+----------+--------+-------------+--------+\n",
      "|2011-01-01 00:00:00|72494023234| 37.6197|-122.3647|SAN FRANCISCO INT...|       0.0|    89.0|          1.0|    Rain|\n",
      "|2011-01-01 01:00:00|72494023234| 37.6197|-122.3647|SAN FRANCISCO INT...|       0.0|    89.0|          1.0|    Rain|\n",
      "|2011-01-01 02:00:00|72494023234| 37.6197|-122.3647|SAN FRANCISCO INT...|       0.0|    89.0|          1.0|    Rain|\n",
      "|2011-01-01 03:00:00|72494023234| 37.6197|-122.3647|SAN FRANCISCO INT...|      15.0|    89.0|          1.0|    Rain|\n",
      "|2011-01-01 04:00:00|72494023234| 37.6197|-122.3647|SAN FRANCISCO INT...|      31.0|    89.0|          1.0|    Rain|\n",
      "+-------------------+-----------+--------+---------+--------------------+----------+--------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- LATITUDE: string (nullable = true)\n",
      " |-- LONGITUDE: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- air_temp: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- wth_type: string (nullable = true)\n",
      "\n",
      "23/03/13 03:33:26 WARN TaskSetManager: Stage 436 contains a task of very large size (6272 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "420615"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather4.show(5)\n",
    "weather4.printSchema()\n",
    "weather4.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f303e-2708-41a0-add8-1cfaa32ea103",
   "metadata": {},
   "source": [
    "# Joining Weather and unique station Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def63ddd-8a94-48d0-a0b9-453d317b8006",
   "metadata": {},
   "source": [
    "We will first join the weather dataset with station data set to get the distance between each bart station to its closet weather station. Afterwards, we will join this data set to the ridership dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "92881620-5a8d-495b-b0d7-38fd64dd889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns \n",
    "weather4 = weather4.withColumnRenamed(\"LATITUDE\",\"latitude_wthr\").withColumnRenamed(\"LONGITUDE\",\"longitude_wthr\")\\\n",
    "                   .withColumnRenamed(\"STATION\",\"station\").withColumnRenamed('NAME','wthr_station_name')\\\n",
    "                   .withColumn('wthr_station_name', f.when(weather4['wthr_station_name'] == 'SAN JOSE, CA US', 'SAN JOSE INTERNATIONAL AIRPORT, CA US').otherwise(weather4['wthr_station_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "68cb58d8-2fb0-442c-84e4-7ef702684fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- latitude_wthr: string (nullable = true)\n",
      " |-- longitude_wthr: string (nullable = true)\n",
      " |-- wthr_station_name: string (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- air_temp: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- wth_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "782d4828-78e9-41db-81c2-5fbe52fa4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_weather_joined to the station data\n",
    "df_station_full1 = df_station_full.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "#parititioning by index \n",
    "w=Window().partitionBy(\"index\")\n",
    "\n",
    "#calculating the Euclidean distance between Bart stations and weather stations\n",
    "#filtering by min distance from each bart station to weather stations \n",
    "station_weather_joined = df_station_full1.join(weather4.withColumnRenamed(\"LATITUDE\",\"latitude_wthr\")\\\n",
    "            .withColumnRenamed(\"LONGITUDE\",\"longitude_wthr\")) \\\n",
    "            .withColumn(\"distance\",f.sqrt(f.pow(f.col(\"latitude\")-f.col(\"latitude_wthr\"),2)+\\\n",
    "                                   f.pow(f.col(\"longitude\")-f.col(\"longitude_wthr\"),2)))\\\n",
    "            .withColumn(\"min_distance\", f.min(\"distance\").over(w))\\\n",
    "            .filter('distance=min_distance') \\\n",
    "            .drop('index','distance','min_distance','latitude','station_name','longitude')\n",
    "\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "82154e42-6ebc-44f1-b608-506b9b3a61b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 04:44:25 WARN TaskSetManager: Stage 359 contains a task of very large size (6273 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# writing to parquet\n",
    "# station_weather_joined.write.parquet('s3a://w210-bucket/data_wrangling/station_weather_cleanedUp.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "85e96ef6-6783-47f2-967f-ccbcbddade30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the station_weather_joined data\n",
    "station_weather_cleaned = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/station_weather_cleanedUp.parquet\")\n",
    "\n",
    "#getting rid of duplicate airport name\n",
    "station_weather_cleaned = station_weather_cleaned.filter(col('wthr_station_name') != 'SAN JOSE, CA US')\n",
    "#finding the closing the weather station and bart stations\n",
    "station_weather_closest = station_weather_cleaned.select('abbreviation','station').dropDuplicates()\n",
    "\n",
    "#joining station_weather_closest to the weather dataset\n",
    "\n",
    "final_weather = weather4.join(station_weather_closest,['station'],'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d85757ad-f47a-4006-bb83-dec1958df935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 04:44:53 WARN TaskSetManager: Stage 366 contains a task of very large size (6272 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# writing to parquet\n",
    "# final_weather.write.parquet('s3a://w210-bucket/data_wrangling/final_weather.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "35483704-5198-4117-af75-ccf1085d41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the station_weather_joined data\n",
    "final_weather = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/final_weather.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5cef73ec-b7bd-40f6-b606-563a0957b9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>ts</th>\n",
       "      <th>latitude_wthr</th>\n",
       "      <th>longitude_wthr</th>\n",
       "      <th>wthr_station_name</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wth_type</th>\n",
       "      <th>abbreviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station  ts  latitude_wthr  longitude_wthr  wthr_station_name  wind_speed  \\\n",
       "0        0   0              0               0                  0           0   \n",
       "\n",
       "   air_temp  precipitation  wth_type  abbreviation  \n",
       "0         0              0         0             0  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for nulls in station_weather_cleaned data\n",
    "missing_counts = final_weather.select([count(when(col(c).isNull(), c)).alias(c) for c in final_weather.columns]).toPandas()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "abb2234c-6e02-49a8-a3e8-6b75a8d948f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weather = final_weather.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "33a39b51-09d8-4538-a310-91348189f4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# # dropping duplicates from df_station_ridership and saving to parquet\n",
    "# df_station_ridership = df_station_ridership.dropDuplicates()\n",
    "# df_station_ridership.write.parquet('s3a://w210-bucket/data_wrangling/df_station_ridership.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0879ebd7-9877-4b87-a440-a20fd870f03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   wthr_station_name|\n",
      "+--------------------+\n",
      "|CONCORD BUCHANAN ...|\n",
      "|LIVERMORE MUNICIP...|\n",
      "|SAN JOSE INTERNAT...|\n",
      "|SAN FRANCISCO INT...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_weather.select('wthr_station_name').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3286792a-c7c5-4f21-9f19-9e9f26197ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the df_station_ridership data\n",
    "df_station_ridership = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_station_ridership.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7a020041-16fe-4922-b6d3-81e86d3d81df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- latitude_wthr: string (nullable = true)\n",
      " |-- longitude_wthr: string (nullable = true)\n",
      " |-- wthr_station_name: string (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- air_temp: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- wth_type: string (nullable = true)\n",
      " |-- abbreviation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e85d0e70-bff3-4fe2-87d5-959996aceff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### joining station_weather dataset with the ridership dataset\n",
    "# # renaming the column to make it easier to join\n",
    "final_weather = final_weather.withColumnRenamed('abbreviation','origin')\n",
    "\n",
    "# df_joined1 = df_station_ridership.join(station_weather_cleaned1,(df_station_ridership['ts'] == station_weather_cleaned1['ts1']) & \\\n",
    "#                                        (df_station_ridership.origin == station_weather_cleaned1.abbreviation),'left').drop('ts1','abbreviation')\n",
    "\n",
    "df_joined1 = df_station_ridership.join(final_weather,['origin','ts'],'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7f97e82f-a1ee-472f-9bea-962d3859efcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#writing to parquet\n",
    "# df_joined1.write.parquet('s3a://w210-bucket/data_wrangling/df_joined1.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "051e7ea4-aff0-4139-94c9-270b857e76ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>ts</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>destination</th>\n",
       "      <th>ridership_number</th>\n",
       "      <th>origin-des</th>\n",
       "      <th>station_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>quarter</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>station</th>\n",
       "      <th>latitude_wthr</th>\n",
       "      <th>longitude_wthr</th>\n",
       "      <th>wthr_station_name</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wth_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin  ts  date  hour  destination  ridership_number  origin-des  \\\n",
       "0       0   0     0     0            0                 0           0   \n",
       "\n",
       "   station_name  longitude  latitude  ...  quarter  day_of_week  station  \\\n",
       "0             0          0         0  ...        0            0   261669   \n",
       "\n",
       "   latitude_wthr  longitude_wthr  wthr_station_name  wind_speed  air_temp  \\\n",
       "0         261669          261669             261669      261669    261669   \n",
       "\n",
       "   precipitation  wth_type  \n",
       "0         261669    261669  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading df_joined1 and counting missing values \n",
    "df_joined1 = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_joined1.parquet\")\n",
    "\n",
    "# checking for nulls in df_joined1 data\n",
    "missing_counts = df_joined1.select([count(when(col(c).isNull(), c)).alias(c) for c in df_joined1.columns]).toPandas()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8d3d766-1451-4f3c-bdaf-cfdb4200e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping null values. About 0.24% of the data\n",
    "df_joined1 = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_joined1.parquet\")\n",
    "df_joined1 = df_joined1.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae1368-3712-4250-a4ce-681d1f8ca9e7",
   "metadata": {},
   "source": [
    "### joining weather based on destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7280a3e9-48e1-4da5-9510-43f12bc2c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming weather columns for the origin\n",
    "df_joined1_renamed = df_joined1.withColumnRenamed('station', 'station_origin')\\\n",
    "                       .withColumnRenamed('wthr_station_name', 'wthr_station_origin')\\\n",
    "                       .withColumnRenamed('latitude_wthr', 'latitude_wthr_origin')\\\n",
    "                       .withColumnRenamed('longitude_wthr', 'longitude_wthr_origin')\\\n",
    "                       .withColumnRenamed('wind_speed', 'wind_speed_origin')\\\n",
    "                       .withColumnRenamed('air_temp', 'air_temp_origin')\\\n",
    "                       .withColumnRenamed('precipitation', 'precipitation_origin')\\\n",
    "                       .withColumnRenamed('wth_type', 'wth_type_origin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f174fb5-7720-4467-8d27-4d96a04a6a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- origin: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      " |-- ridership_number: integer (nullable = true)\n",
      " |-- origin-des: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- quarter: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- station_origin: string (nullable = true)\n",
      " |-- latitude_wthr_origin: string (nullable = true)\n",
      " |-- longitude_wthr_origin: string (nullable = true)\n",
      " |-- wthr_station_origin: string (nullable = true)\n",
      " |-- wind_speed_origin: double (nullable = true)\n",
      " |-- air_temp_origin: double (nullable = true)\n",
      " |-- precipitation_origin: double (nullable = true)\n",
      " |-- wth_type_origin: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined1_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4b9a158-c70e-4f51-a03b-a98f8bcab149",
   "metadata": {},
   "outputs": [],
   "source": [
    "### joining station_weather dataset with the ridership dataset\n",
    "# # renaming the column to make it easier to join\n",
    "\n",
    "# reading the final_weather data\n",
    "final_weather = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/final_weather.parquet\")\n",
    "\n",
    "#dropping duplicates \n",
    "final_weather = final_weather.dropDuplicates()\n",
    "\n",
    "final_weather = final_weather.withColumnRenamed('abbreviation','destination')\n",
    "\n",
    "df_joined2 = df_joined1_renamed.join(final_weather,['destination','ts'],'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3b65cd9-6d44-44be-b5e1-922146b62e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 05:13:29 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving to S3 as a parquet file \n",
    "# df_joined2.write.parquet('s3a://w210-bucket/data_wrangling/df_joined2.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d175ec36-1019-4e72-9764-dfa5ebe336d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the first joined data\n",
    "df_joined2 = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_joined2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "570da6d3-ceaa-4e89-a3fa-4493d3e25f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- destination: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- ridership_number: integer (nullable = true)\n",
      " |-- origin-des: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- quarter: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- station_origin: string (nullable = true)\n",
      " |-- latitude_wthr_origin: string (nullable = true)\n",
      " |-- longitude_wthr_origin: string (nullable = true)\n",
      " |-- wthr_station_origin: string (nullable = true)\n",
      " |-- wind_speed_origin: double (nullable = true)\n",
      " |-- air_temp_origin: double (nullable = true)\n",
      " |-- precipitation_origin: double (nullable = true)\n",
      " |-- wth_type_origin: string (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- latitude_wthr: string (nullable = true)\n",
      " |-- longitude_wthr: string (nullable = true)\n",
      " |-- wthr_station_name: string (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- air_temp: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- wth_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99bb7b77-346c-4cba-aef9-f57c716234c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109442517"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1118f05-cfc5-4e9c-8d2c-e11ac7627b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination</th>\n",
       "      <th>ts</th>\n",
       "      <th>origin</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>ridership_number</th>\n",
       "      <th>origin-des</th>\n",
       "      <th>station_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>precipitation_origin</th>\n",
       "      <th>wth_type_origin</th>\n",
       "      <th>station</th>\n",
       "      <th>latitude_wthr</th>\n",
       "      <th>longitude_wthr</th>\n",
       "      <th>wthr_station_name</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wth_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   destination  ts  origin  date  hour  ridership_number  origin-des  \\\n",
       "0            0   0       0     0     0                 0           0   \n",
       "\n",
       "   station_name  longitude  latitude  ...  precipitation_origin  \\\n",
       "0             0          0         0  ...                     0   \n",
       "\n",
       "   wth_type_origin  station  latitude_wthr  longitude_wthr  wthr_station_name  \\\n",
       "0                0   226936         226936          226936             226936   \n",
       "\n",
       "   wind_speed  air_temp  precipitation  wth_type  \n",
       "0      226936    226936         226936    226936  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for nulls in df_joined1 data\n",
    "missing_counts = df_joined2.select([count(when(col(c).isNull(), c)).alias(c) for c in df_joined2.columns]).toPandas()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47a4fbf6-eb04-4f39-a74f-c40f8b31be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined2_1 = df_joined2.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfb6d199-0758-4d07-8b18-ffde38821f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination</th>\n",
       "      <th>ts</th>\n",
       "      <th>origin</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>ridership_number</th>\n",
       "      <th>origin-des</th>\n",
       "      <th>station_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>precipitation_origin</th>\n",
       "      <th>wth_type_origin</th>\n",
       "      <th>station</th>\n",
       "      <th>latitude_wthr</th>\n",
       "      <th>longitude_wthr</th>\n",
       "      <th>wthr_station_name</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wth_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   destination  ts  origin  date  hour  ridership_number  origin-des  \\\n",
       "0            0   0       0     0     0                 0           0   \n",
       "\n",
       "   station_name  longitude  latitude  ...  precipitation_origin  \\\n",
       "0             0          0         0  ...                     0   \n",
       "\n",
       "   wth_type_origin  station  latitude_wthr  longitude_wthr  wthr_station_name  \\\n",
       "0                0        0              0               0                  0   \n",
       "\n",
       "   wind_speed  air_temp  precipitation  wth_type  \n",
       "0           0         0              0         0  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for nulls in df_joined1 data\n",
    "missing_counts = df_joined2_1.select([count(when(col(c).isNull(), c)).alias(c) for c in df_joined2_1.columns]).toPandas()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e49ca11-849d-4765-aa8c-8741550a6775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109215581"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined2_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69c58d8c-a6c7-483b-9d8c-d088ac7a8b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#getting rid of nulls and resaving the file\n",
    "# df_joined2_1.write.parquet('s3a://w210-bucket/data_wrangling/df_joined2_1.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c41694-dd58-4160-a42b-0d13df8139a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
