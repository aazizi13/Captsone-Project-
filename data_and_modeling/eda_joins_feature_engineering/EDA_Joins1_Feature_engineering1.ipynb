{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3c3cd2-c6e1-43e9-8f16-ff0499650d58",
   "metadata": {},
   "source": [
    "# Data Wrangling Notebook 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d8bdfa2-819d-4ba4-80fd-7f704b660808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import split, slice, count, when, expr, isnan, isnull\n",
    "from pyspark.sql.functions import date_format, to_timestamp, concat, unix_timestamp, substring, lit\n",
    "from pyspark.sql.functions import col, month, quarter, dayofweek, year\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType\n",
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "from pyspark.sql.window import Window\n",
    "import configparser\n",
    "import findspark\n",
    "import lxml\n",
    "from datetime import timedelta\n",
    "from pandas.tseries.offsets import BDay\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa25a4-0309-4362-a308-2b0d1d50e207",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reading From S3 Bucket using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db130b-4b36-4d96-87f5-f0eea6ed9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# This code block is used for pulling data from S3 using boto3 library\n",
    "############# Note: Not necessary for this project\n",
    "\n",
    "#getting relevant information for the S3 bucket\n",
    "\n",
    "# AWS_S3_BUCKET='w210-bucket'\n",
    "# AWS_S3_REGION='us-east-1'\n",
    "# AWS_PROFILE_NAME='default'\n",
    "# session = boto3.Session(profile_name=AWS_PROFILE_NAME)\n",
    "# s3 = session.resource('s3')\n",
    "# s3_client = session.client('s3',region_name=AWS_S3_REGION)\n",
    "# my_bucket = s3.Bucket(AWS_S3_BUCKET)\n",
    "\n",
    "# #printing all the files in ridership directory \n",
    "# for objects in my_bucket.objects.filter(Prefix=\"ridership/\"):\n",
    "#     print(objects.key)\n",
    "\n",
    "# #printing all the files in weather directory \n",
    "# for objects in my_bucket.objects.filter(Prefix=\"weather/\"):\n",
    "#     print(objects.key)\n",
    "\n",
    "# #reading one file only as pandas df\n",
    "# obj = s3_client.get_object(Bucket= AWS_S3_BUCKET, Key= \"ridership/date-hour-soo-dest-2011.csv\") \n",
    "# # get object and file (key) from bucket\n",
    "# initial_df = pd.read_csv(obj['Body'], header=None) \n",
    "# initial_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef685327-0b87-4a4e-a1b1-bd046c494607",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reading From S3 Bucket using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35af90ae-44cd-4deb-bcc6-83c480a410cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Starting Pyspark Session\n",
    "\n",
    "# spark = SparkSession.builder\\\n",
    "#                     .config('spark.master','local[*]')\\\n",
    "#                     .config('spark.add.name','S3app')\\\n",
    "#                     .config('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-common:3.3.4')\\\n",
    "#                     .getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .config('spark.master','local[*]')\\\n",
    "                    .config('spark.add.name','S3app')\\\n",
    "                    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "                    .config('spark.jars.packages','org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-common:3.3.4')\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8364aa13-912e-4382-a45c-1e2856ee137c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-13-189.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcb10fe6130>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49bf5dc4-14c5-4b00-924b-28791908fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configuring Pyspark to read data from S3 Bucket. \n",
    "\n",
    "findspark.init()\n",
    "config = configparser.ConfigParser()\n",
    "# AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "# AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "aws_profile = 'default'\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "access_id = config.get(aws_profile, \"aws_access_key_id\") \n",
    "access_key = config.get(aws_profile, \"aws_secret_access_key\")\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)\n",
    "# spark.conf.set(\"spark.sql.broadcastTimeout\", 3000)\n",
    "\n",
    "# spark.conf.set(\"spark.driver.maxResultSize\",8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sc=spark.sparkContext\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.committer.name\",\"magic\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.awsAccessKeyId\", access_id)\n",
    "hadoop_conf.set(\"fs.s3a.awsSecretAccessKey\", access_key)\n",
    "hadoop_conf.set('spark.sql.files.maxPartitionBytes','134217728')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "29843d0f-4c94-4606-a44d-76ca3cead52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa361767-9a08-4ec4-9f0a-119bed90acdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ridership Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549a440-2661-4ac8-b145-568a6ec34447",
   "metadata": {},
   "source": [
    "### In this section of Notebook, we will read all the ridership data and do data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "699e4985-af69-4191-bc39-35def970a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading all the ridership data\n",
    "ridership_2011 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2011.csv\")\n",
    "ridership_2012 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2012.csv\")\n",
    "ridership_2013 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2013.csv\")\n",
    "ridership_2014 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2014.csv\")\n",
    "ridership_2015 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2015.csv\")\n",
    "ridership_2016 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2016.csv\")\n",
    "ridership_2017 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2017.csv\")\n",
    "ridership_2018 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2018.csv\")\n",
    "ridership_2019 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2019.csv\")\n",
    "ridership_2020 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2020.csv\")\n",
    "ridership_2021 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2021.csv\")\n",
    "ridership_2022 = spark.read.option('header',False).csv(\"s3a://w210-bucket/ridership/date-hour-soo-dest-2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e1fa0900-620e-4031-aaa3-808b059f8c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      " |-- ridership_number: integer (nullable = true)\n",
      " |-- origin-des: string (nullable = true)\n",
      "\n",
      "+----------+----+------+-----------+----------------+----------+\n",
      "|      date|hour|origin|destination|ridership_number|origin-des|\n",
      "+----------+----+------+-----------+----------------+----------+\n",
      "|2011-01-01|   0|  12TH|       12TH|               1| 12TH-12TH|\n",
      "|2011-01-01|   0|  12TH|       16TH|               1| 12TH-16TH|\n",
      "|2011-01-01|   0|  12TH|       24TH|               3| 12TH-24TH|\n",
      "|2011-01-01|   0|  12TH|       ASHB|               2| 12TH-ASHB|\n",
      "|2011-01-01|   0|  12TH|       BAYF|               5| 12TH-BAYF|\n",
      "|2011-01-01|   0|  12TH|       CIVC|               3| 12TH-CIVC|\n",
      "|2011-01-01|   0|  12TH|       COLS|               1| 12TH-COLS|\n",
      "|2011-01-01|   0|  12TH|       CONC|               2| 12TH-CONC|\n",
      "|2011-01-01|   0|  12TH|       DALY|               1| 12TH-DALY|\n",
      "|2011-01-01|   0|  12TH|       DBRK|               4| 12TH-DBRK|\n",
      "+----------+----+------+-----------+----------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all the ridership data from 2011 to 2022\n",
    "df_ridership_all = ridership_2011.union(ridership_2012).union(ridership_2013).union(ridership_2014)\\\n",
    "                             .union(ridership_2015).union(ridership_2016).union(ridership_2017)\\\n",
    "                             .union(ridership_2018).union(ridership_2019).union(ridership_2020)\\\n",
    "                             .union(ridership_2021).union(ridership_2022)\n",
    "\n",
    "#Rename columns and creating a new column called origin-destination pair \n",
    "df_ridership_all =df_ridership_all.withColumnRenamed('_c0',\"date\") \\\n",
    "        .withColumnRenamed('_c1','hour') \\\n",
    "        .withColumnRenamed('_c2','origin') \\\n",
    "        .withColumnRenamed('_c3','destination') \\\n",
    "        .withColumnRenamed('_c4','ridership_number') \\\n",
    "        .withColumn('origin-des', concat(col('origin'), lit('-'), col('destination')))\n",
    "\n",
    "# change data schema\n",
    "df_ridership_all = df_ridership_all.withColumn(\"date\",col(\"date\").cast(DateType())) \\\n",
    "    .withColumn(\"hour\",col(\"hour\").cast(IntegerType())) \\\n",
    "    .withColumn(\"ridership_number\",col(\"ridership_number\").cast(IntegerType()))\n",
    "\n",
    "# print the schema and the first 5 rows\n",
    "df_ridership_all.printSchema()\n",
    "df_ridership_all.show(10)\n",
    "# df_ridership_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b9c5810d-ca0b-45e6-8066-f64644451900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abbreviation: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      "\n",
      "+------------+--------------------+-----------+---------+\n",
      "|abbreviation|        station_name|  longitude| latitude|\n",
      "+------------+--------------------+-----------+---------+\n",
      "|        12TH|12th St. Oakland ...|-122.271450|37.803768|\n",
      "|        16TH|16th St. Mission ...|-122.419694|37.765062|\n",
      "|        19TH|19th St. Oakland ...|-122.268602|37.808350|\n",
      "|        24TH|24th St. Mission ...|-122.418143|37.752470|\n",
      "|        ASHB|        Ashby (ASHB)|-122.270062|37.852803|\n",
      "|        BALB|  Balboa Park (BALB)|-122.447506|37.721585|\n",
      "|        BAYF|     Bay Fair (BAYF)|-122.126514|37.696924|\n",
      "|        CAST|Castro Valley (CAST)|-122.075602|37.690746|\n",
      "|        CIVC|Civic Center/UN P...|-122.414123|37.779732|\n",
      "|        COLS|Coliseum/Oakland ...|-122.196869|37.753661|\n",
      "+------------+--------------------+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### read the station data to get station abbreviations, Longitude, and Latitude\n",
    "\n",
    "df_station = spark.read.option(\"header\",True) \\\n",
    "           .csv(\"s3a://w210-bucket/ridership/station_bart.csv\")\n",
    "\n",
    "#rename columns\n",
    "df_station = df_station.withColumnRenamed('Abbreviation','abbreviation') \\\n",
    "                        .withColumnRenamed('Location','location')\\\n",
    "                        .withColumnRenamed('Name','station_name').drop('description').drop('name')\n",
    "\n",
    "#split longitude and latitude into separate columns\n",
    "df_station = df_station.withColumn(\"longitude\", split(col(\"location\"), \",\").getItem(0)) \\\n",
    "                         .withColumn(\"latitude\", split(col(\"location\"), \",\").getItem(1)).drop('location')\n",
    "\n",
    "# print schema and the first 5 rows\n",
    "df_station.printSchema()\n",
    "df_station.show(10)\n",
    "# df_station.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "29b8f5aa-eb42-4563-adb5-c71ac5e33495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>station_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   abbreviation  station_name  longitude  latitude\n",
       "0             0             0          0         0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking null values in df_station data\n",
    "missing_counts = df_station.select([count(when(col(c).isNull(), c)).alias(c) for c in df_station.columns]).toPandas()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce4fa5a6-17c8-48b7-bc06-42fa0e9013ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique number of bart stations in df_station: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:======================================================> (94 + 2) / 96]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique number of bart stations in df_ridership_all: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# looking at unique bart stations in df_station and ridership data\n",
    "print(f\"Unique number of bart stations in df_station: {df_station.select('abbreviation').distinct().count()}\")\n",
    "print(f\"Unique number of bart stations in df_ridership_all: {df_ridership_all.select('origin').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3a889153-0210-4b6a-b3db-3010f33e2b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# finding out which bart stations don't exists\n",
    "stations_df_stations = df_station.select('abbreviation').distinct().toPandas()\n",
    "stations_df_ridership_all = df_ridership_all.select('origin').distinct().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "16a6417f-8c78-4723-9f09-ac3a708623fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ANTC', 'PCTR', 'BERY', 'MLPT']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df_stations_list = stations_df_stations['abbreviation'].tolist()\n",
    "stations_df_ridership_all_list = stations_df_ridership_all['origin'].tolist()\n",
    "\n",
    "missing_bart_stations = [x for x in stations_df_ridership_all_list if x not in  stations_df_stations_list]\n",
    "missing_bart_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1d12e140-d89b-451c-b41a-30a6b5862de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the missing station data to the df_station data\n",
    "columns = ['abbreviation', 'station_name', 'longitude', 'latitude']\n",
    "data = [\\\n",
    "        ('ANTC','Antioch Station', '-121.780320', '37.996012'),\\\n",
    "        ('PCTR','Pittsburg Center Station', '-121.888538', '38.018200'),\\\n",
    "        ('BERY','Berryessa Bart Station', '-121.874689', '37.368572'),\\\n",
    "        ('MLPT','Milpitas Bart Station', '-121.890621', '37.409878')]\n",
    "\n",
    "missing_stations = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_station_full = df_station.union(missing_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4d624da9-16ce-481e-899a-07c0dfdcc00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      " |-- ridership_number: integer (nullable = true)\n",
      " |-- origin-des: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- quarter: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 344:===========>   (76 + 8) / 96][Stage 345:>                (0 + 0) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+-----------+----------------+----------+--------------------+-----------+---------+----+-----+-------+-----------+-------------------+\n",
      "|      date|hour|origin|destination|ridership_number|origin-des|        station_name|  longitude| latitude|year|month|quarter|day_of_week|                 ts|\n",
      "+----------+----+------+-----------+----------------+----------+--------------------+-----------+---------+----+-----+-------+-----------+-------------------+\n",
      "|2011-01-01|   0|  12TH|       12TH|               1| 12TH-12TH|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       16TH|               1| 12TH-16TH|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       24TH|               3| 12TH-24TH|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       ASHB|               2| 12TH-ASHB|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       BAYF|               5| 12TH-BAYF|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       CIVC|               3| 12TH-CIVC|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       COLS|               1| 12TH-COLS|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       CONC|               2| 12TH-CONC|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       DALY|               1| 12TH-DALY|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "|2011-01-01|   0|  12TH|       DBRK|               4| 12TH-DBRK|12th St. Oakland ...|-122.271450|37.803768|2011|    1|      1|          7|2011-01-01 00:00:00|\n",
      "+----------+----+------+-----------+----------------+----------+--------------------+-----------+---------+----+-----+-------+-----------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### join ridership data to station data based on origin\n",
    "\n",
    "df_station_ridership = df_ridership_all.join(df_station_full,df_ridership_all.origin ==  df_station_full.abbreviation,\"left\").drop('abbreviation')\n",
    "\n",
    "#extract month, quarter, and day_of_week from the date column\n",
    "df_station_ridership = df_station_ridership.withColumn('year', year(df_station_ridership.date)) \\\n",
    "                                            .withColumn('month', month(df_station_ridership.date)) \\\n",
    "                                            .withColumn('quarter', quarter(df_station_ridership.date)) \\\n",
    "                                            .withColumn('day_of_week', dayofweek(df_station_ridership.date)) \n",
    "\n",
    "#set timeparser policy to legacy \n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "#create timestamp\n",
    "df_station_ridership = df_station_ridership.withColumn(\"ts\",f.to_timestamp(f.concat(\"date\",\"hour\"),\"yyyy-MM-ddHH\"))\n",
    "\n",
    "#change data schema and print first 5 rows\n",
    "df_station_ridership = df_station_ridership.withColumn(\"date\",col(\"date\").cast(DateType())) \\\n",
    "    .withColumn(\"hour\",col(\"hour\").cast('string')) \\\n",
    "    .withColumn(\"year\",col(\"year\").cast('string')) \\\n",
    "    .withColumn(\"month\",col(\"month\").cast('string')) \\\n",
    "    .withColumn(\"quarter\",col(\"quarter\").cast('string')) \\\n",
    "    .withColumn(\"day_of_week\",col(\"day_of_week\").cast('string'))\n",
    "    # .withColumn('ts_long', col('ts').cast('long'))\n",
    "\n",
    "df_station_ridership.printSchema()\n",
    "df_station_ridership.show(10)\n",
    "# df_station_ridership.count()\n",
    "# df_station_ridership.select('ts_long').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "05639b37-4f93-479d-b892-ae26ba9bacb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>ridership_number</th>\n",
       "      <th>origin-des</th>\n",
       "      <th>station_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date  hour  origin  destination  ridership_number  origin-des  \\\n",
       "0     0     0       0            0                 0           0   \n",
       "\n",
       "   station_name  longitude  latitude  year  month  quarter  day_of_week  ts  \n",
       "0             0          0         0     0      0        0            0   0  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for nulls in df_station_ridership data\n",
    "missing_counts = df_station_ridership.select([count(when(col(c).isNull(), c)).alias(c) for c in df_station_ridership.columns]).toPandas()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f8922-5493-4f63-8909-2f8d8c10df34",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205278b0-91cf-40a9-aaf0-e6dbb8d1cf2b",
   "metadata": {},
   "source": [
    "### In this section, we will read all the weather data and do some data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e56b9752-59bc-49fa-a224-849d30cacd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custome Functions\n",
    "\n",
    "def weather_read(df):\n",
    "    \"\"\"\n",
    "    Input = file path in S3 Bucket\n",
    "    Ouput = PySpark DataFrame\n",
    "    \"\"\"\n",
    "    path = df\n",
    "    df = spark.read.option('header',True).csv(path)\n",
    "    return df\n",
    "\n",
    "def weather_cleanup(df):\n",
    "    \"\"\"\n",
    "    Input: Weather station PySpark DataFrame\n",
    "    Operation: selects columns, gets rid of minutes and zeroes out the hours,\n",
    "               creates timestamp, and drops duplicate rows for timestamps\n",
    "               \n",
    "    Output: Pyspark DataFrame\n",
    "    \"\"\"\n",
    "    df = df.withColumn('date1',concat(substring('DATE',1,10), lit(' '), substring('DATE', 12,2), lit(':00:00'))) \\\n",
    "           .withColumn('ts', f.to_timestamp('date1',\"yyyy-MM-dd HH:mm:ss\")).drop('date1').dropDuplicates(['ts'])\\\n",
    "           .select('ts','STATION','DATE','SOURCE','LATITUDE','LONGITUDE','ELEVATION','NAME',\n",
    "                   'REPORT_TYPE','CALL_SIGN','QUALITY_CONTROL','WND','CIG','VIS','TMP','DEW','SLP','AA1','AT1','AU1')\n",
    "           # .withColumn('ts_long', col('ts').cast('long'))\n",
    "    return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a2e2ba4f-616e-44ce-9c24-ac9200d50843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# San Franciso station\n",
    "sf_2011 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2011.csv\")\n",
    "sf_2011 = weather_cleanup(sf_2011)\n",
    "sf_2012 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2012.csv\")\n",
    "sf_2012 = weather_cleanup(sf_2012)\n",
    "sf_2013 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2013.csv\")\n",
    "sf_2013 = weather_cleanup(sf_2013)\n",
    "sf_2014 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2014.csv\")\n",
    "sf_2014 = weather_cleanup(sf_2014)\n",
    "sf_2015 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2015.csv\")\n",
    "sf_2015 = weather_cleanup(sf_2015)\n",
    "sf_2016 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2016.csv\")\n",
    "sf_2016 = weather_cleanup(sf_2016)\n",
    "sf_2017 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2017.csv\")\n",
    "sf_2017 = weather_cleanup(sf_2017)\n",
    "sf_2018 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2018.csv\")\n",
    "sf_2018 = weather_cleanup(sf_2018)\n",
    "sf_2019 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2019.csv\")\n",
    "sf_2019 = weather_cleanup(sf_2019)\n",
    "sf_2020 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2020.csv\")\n",
    "sf_2020 = weather_cleanup(sf_2020)\n",
    "sf_2021 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2021.csv\")\n",
    "sf_2021 = weather_cleanup(sf_2021)\n",
    "sf_2022 = weather_read(\"s3a://w210-bucket/weather/weather_sf_2022.csv\")\n",
    "sf_2022 = weather_cleanup(sf_2022)\n",
    "\n",
    "# Concord Station\n",
    "concord_2011 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2011.csv\")\n",
    "concord_2011 = weather_cleanup(concord_2011)\n",
    "concord_2012 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2012.csv\")\n",
    "concord_2012 = weather_cleanup(concord_2012)\n",
    "concord_2013 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2013.csv\")\n",
    "concord_2013 = weather_cleanup(concord_2013)\n",
    "concord_2014 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2014.csv\")\n",
    "concord_2014 = weather_cleanup(concord_2014)\n",
    "concord_2015 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2015.csv\")\n",
    "concord_2015 = weather_cleanup(concord_2015)\n",
    "concord_2016 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2016.csv\")\n",
    "concord_2016 = weather_cleanup(concord_2016)\n",
    "concord_2017 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2017.csv\")\n",
    "concord_2017 = weather_cleanup(concord_2017)\n",
    "concord_2018 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2018.csv\")\n",
    "concord_2018 = weather_cleanup(concord_2018)\n",
    "concord_2019 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2019.csv\")\n",
    "concord_2019 = weather_cleanup(concord_2019)\n",
    "concord_2020 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2020.csv\")\n",
    "concord_2020 = weather_cleanup(concord_2020)\n",
    "concord_2021 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2021.csv\")\n",
    "concord_2021 = weather_cleanup(concord_2021)\n",
    "concord_2022 = weather_read(\"s3a://w210-bucket/weather/weather_concord_2022.csv\")\n",
    "concord_2022 = weather_cleanup(concord_2022)\n",
    "\n",
    "\n",
    "# livermore Station\n",
    "livermore_2011 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2011.csv\")\n",
    "livermore_2011 = weather_cleanup(livermore_2011)\n",
    "livermore_2012 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2012.csv\")\n",
    "livermore_2012 = weather_cleanup(livermore_2012)\n",
    "livermore_2013 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2013.csv\")\n",
    "livermore_2013 = weather_cleanup(livermore_2013)\n",
    "livermore_2014 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2014.csv\")\n",
    "livermore_2014 = weather_cleanup(livermore_2014)\n",
    "livermore_2015 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2015.csv\")\n",
    "livermore_2015 = weather_cleanup(livermore_2015)\n",
    "livermore_2016 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2016.csv\")\n",
    "livermore_2016 = weather_cleanup(livermore_2016)\n",
    "livermore_2017 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2017.csv\")\n",
    "livermore_2017 = weather_cleanup(livermore_2017)\n",
    "livermore_2018 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2018.csv\")\n",
    "livermore_2018 = weather_cleanup(livermore_2018)\n",
    "livermore_2019 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2019.csv\")\n",
    "livermore_2019 = weather_cleanup(livermore_2019)\n",
    "livermore_2020 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2020.csv\")\n",
    "livermore_2020 = weather_cleanup(livermore_2020)\n",
    "livermore_2021 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2021.csv\")\n",
    "livermore_2021 = weather_cleanup(livermore_2021)\n",
    "livermore_2022 = weather_read(\"s3a://w210-bucket/weather/weather_livermore_2022.csv\")\n",
    "livermore_2022 = weather_cleanup(livermore_2022)\n",
    "\n",
    "\n",
    "# San Jose Station\n",
    "sanjose_2011 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2011.csv\")\n",
    "sanjose_2011 = weather_cleanup(sanjose_2011)\n",
    "sanjose_2012 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2012.csv\")\n",
    "sanjose_2012 = weather_cleanup(sanjose_2012)\n",
    "sanjose_2013 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2013.csv\")\n",
    "sanjose_2013 = weather_cleanup(sanjose_2013)\n",
    "sanjose_2014 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2014.csv\")\n",
    "sanjose_2014 = weather_cleanup(sanjose_2014)\n",
    "sanjose_2015 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2015.csv\")\n",
    "sanjose_2015 = weather_cleanup(sanjose_2015)\n",
    "sanjose_2016 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2016.csv\")\n",
    "sanjose_2016 = weather_cleanup(sanjose_2016)\n",
    "sanjose_2017 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2017.csv\")\n",
    "sanjose_2017 = weather_cleanup(sanjose_2017)\n",
    "sanjose_2018 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2018.csv\")\n",
    "sanjose_2018 = weather_cleanup(sanjose_2018)\n",
    "sanjose_2019 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2019.csv\")\n",
    "sanjose_2019 = weather_cleanup(sanjose_2019)\n",
    "sanjose_2020 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2020.csv\")\n",
    "sanjose_2020 = weather_cleanup(sanjose_2020)\n",
    "sanjose_2021 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2021.csv\")\n",
    "sanjose_2021 = weather_cleanup(sanjose_2021)\n",
    "sanjose_2022 = weather_read(\"s3a://w210-bucket/weather/weather_sanjose_2022.csv\")\n",
    "sanjose_2022 = weather_cleanup(sanjose_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d51e1409-d7d3-4345-a008-568863773077",
   "metadata": {},
   "outputs": [],
   "source": [
    "### concatenate weather station data per year\n",
    "\n",
    "# San Francisco\n",
    "sf = sf_2011.union(sf_2012).union(sf_2013).union(sf_2014).union(sf_2015)\\\n",
    "            .union(sf_2016).union(sf_2017).union(sf_2018).union(sf_2019)\\\n",
    "            .union(sf_2020).union(sf_2021).union(sf_2022)\n",
    "\n",
    "# Concord\n",
    "concord = concord_2011.union(concord_2012).union(concord_2013).union(concord_2014).union(concord_2015)\\\n",
    "            .union(concord_2016).union(concord_2017).union(concord_2018).union(concord_2019)\\\n",
    "            .union(concord_2020).union(concord_2021).union(concord_2022)\n",
    "        \n",
    "# Livermore\n",
    "livemore = livermore_2011.union(livermore_2012).union(livermore_2013).union(livermore_2014).union(livermore_2015)\\\n",
    "            .union(livermore_2016).union(livermore_2017).union(livermore_2018).union(livermore_2019)\\\n",
    "            .union(livermore_2020).union(livermore_2021).union(livermore_2022)\n",
    "        \n",
    "# San Jose\n",
    "sanjose = sanjose_2011.union(sanjose_2012).union(sanjose_2013).union(sanjose_2014).union(sanjose_2015)\\\n",
    "            .union(sanjose_2016).union(sanjose_2017).union(sanjose_2018).union(sanjose_2019)\\\n",
    "            .union(sanjose_2020).union(sanjose_2021).union(sanjose_2022)\n",
    "        \n",
    "### concatenate all weather stations \n",
    "weather = sf.union(concord).union(livemore).union(sanjose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ce4c333-7a26-495d-851c-645d8d653d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'San Francisco Count: {sf.count()}')\n",
    "# print(f'Concord Count: {concord.count()}')\n",
    "# print(f'Livermore Count: {livemore.count()}')\n",
    "# print(f'San Jose Count: {sanjose.count()}')\n",
    "# print(f'Total Count: {weather.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad0d2564-81aa-4ce0-8981-2cb88aa64411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- LATITUDE: string (nullable = true)\n",
      " |-- LONGITUDE: string (nullable = true)\n",
      " |-- ELEVATION: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- REPORT_TYPE: string (nullable = true)\n",
      " |-- CALL_SIGN: string (nullable = true)\n",
      " |-- QUALITY_CONTROL: string (nullable = true)\n",
      " |-- WND: string (nullable = true)\n",
      " |-- CIG: string (nullable = true)\n",
      " |-- VIS: string (nullable = true)\n",
      " |-- TMP: string (nullable = true)\n",
      " |-- DEW: string (nullable = true)\n",
      " |-- SLP: string (nullable = true)\n",
      " |-- AA1: string (nullable = true)\n",
      " |-- AT1: string (nullable = true)\n",
      " |-- AU1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3c105ad6-9620-4861-ad24-c02f981fae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 224:>  (0 + 2) / 2][Stage 225:>  (0 + 2) / 2][Stage 226:>  (0 + 2) / 2]2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 04:19:50 WARN DAGScheduler: Broadcasting large task binary with size 1345.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# extracting needed columns from existing columns \n",
    "weather2 = weather.withColumn('wind_speed',f.split(weather['WND'], ',').getItem(3).cast('integer')) \\\n",
    "                  .withColumn('air_temp',f.split(weather['TMP'], ',').getItem(0).cast('integer')) \\\n",
    "                  .withColumn('precipitation',f.split(weather['AA1'], ',').getItem(0).cast('integer')) \\\n",
    "                  .withColumn('wth_type',f.split(weather['AU1'], ',').getItem(2).cast('string'))\n",
    "\n",
    "#putting null for values of 999, 99 and so on based on the documentation\n",
    "# dropping unnessary columns \n",
    "\n",
    "weather2 = weather2.withColumn('air_temp', f.when(weather2['air_temp'] == 9999, None).otherwise(weather2['air_temp'])) \\\n",
    "                   .withColumn('wind_speed', f.when(weather2['wind_speed'] == 9999, None).otherwise(weather2['wind_speed']))\\\n",
    "                   .withColumn('precipitation', f.when(weather2['precipitation'] == 99, None).otherwise(weather2['precipitation'])) \\\n",
    "                   .drop('REPORT_TYPE','CALL_SIGN','QUALITY_CONTROL','ELEVATION','SOURCE','WND',\n",
    "                         'CIG','VIS','TMP','DEW','SLP','AA1','AT1','AU1','DATE')\n",
    "\n",
    "# converting PySpark dataframe to Pandas Dataframe\n",
    "weather3 = weather2.toPandas()\n",
    "\n",
    "# forward filling for missing values \n",
    "for col in ['wind_speed','air_temp','precipitation']:\n",
    "    weather3[col] = weather3[col].ffill()\n",
    "\n",
    "weather3['precipitation'] = weather3['precipitation'].bfill()\n",
    "    \n",
    "# mapping weather types to values \n",
    "d = {'00': 'no_precipitation', '01': 'Drizzle', '02': 'Rain',\n",
    "     '03': 'Snow', '08': 'hail', '09': None}\n",
    "\n",
    "weather3['wth_type'] = weather3['wth_type'].map(d).ffill()\n",
    "weather3['wth_type'] = weather3['wth_type'].bfill()\n",
    "\n",
    "#converting back to PysSpark Dataframe\n",
    "weather4 = spark.createDataFrame(weather3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04352d82-50c5-48c6-8d83-6dec57fa0dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 03:33:26 WARN TaskSetManager: Stage 435 contains a task of very large size (6272 KiB). The maximum recommended task size is 1000 KiB.\n",
      "+-------------------+-----------+--------+---------+--------------------+----------+--------+-------------+--------+\n",
      "|                 ts|    STATION|LATITUDE|LONGITUDE|                NAME|wind_speed|air_temp|precipitation|wth_type|\n",
      "+-------------------+-----------+--------+---------+--------------------+----------+--------+-------------+--------+\n",
      "|2011-01-01 00:00:00|72494023234| 37.6197|-122.3647|SAN FRANCISCO INT...|       0.0|    89.0|          1.0|    Rain|\n",
      "|2011-01-01 01:00:00|72494023234| 37.6197|-122.3647|SAN FRANCISCO INT...|       0.0|    89.0|          1.0|    Rain|\n",
      "|2011-01-01 02:00:00|72494023234| 37.6197|-122.3647|SAN FRANCISCO INT...|       0.0|    89.0|          1.0|    Rain|\n",
      "|2011-01-01 03:00:00|72494023234| 37.6197|-122.3647|SAN FRANCISCO INT...|      15.0|    89.0|          1.0|    Rain|\n",
      "|2011-01-01 04:00:00|72494023234| 37.6197|-122.3647|SAN FRANCISCO INT...|      31.0|    89.0|          1.0|    Rain|\n",
      "+-------------------+-----------+--------+---------+--------------------+----------+--------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- LATITUDE: string (nullable = true)\n",
      " |-- LONGITUDE: string (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- air_temp: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- wth_type: string (nullable = true)\n",
      "\n",
      "23/03/13 03:33:26 WARN TaskSetManager: Stage 436 contains a task of very large size (6272 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "420615"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather4.show(5)\n",
    "weather4.printSchema()\n",
    "weather4.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f303e-2708-41a0-add8-1cfaa32ea103",
   "metadata": {},
   "source": [
    "# Joining Weather and unique station Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def63ddd-8a94-48d0-a0b9-453d317b8006",
   "metadata": {},
   "source": [
    "We will first join the weather dataset with station data set to get the distance between each bart station to its closet weather station. Afterwards, we will join this data set to the ridership dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "92881620-5a8d-495b-b0d7-38fd64dd889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns \n",
    "weather4 = weather4.withColumnRenamed(\"LATITUDE\",\"latitude_wthr\").withColumnRenamed(\"LONGITUDE\",\"longitude_wthr\")\\\n",
    "                   .withColumnRenamed(\"STATION\",\"station\").withColumnRenamed('NAME','wthr_station_name')\\\n",
    "                   .withColumn('wthr_station_name', f.when(weather4['wthr_station_name'] == 'SAN JOSE, CA US', 'SAN JOSE INTERNATIONAL AIRPORT, CA US').otherwise(weather4['wthr_station_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "68cb58d8-2fb0-442c-84e4-7ef702684fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- latitude_wthr: string (nullable = true)\n",
      " |-- longitude_wthr: string (nullable = true)\n",
      " |-- wthr_station_name: string (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- air_temp: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- wth_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "782d4828-78e9-41db-81c2-5fbe52fa4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_weather_joined to the station data\n",
    "df_station_full1 = df_station_full.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "#parititioning by index \n",
    "w=Window().partitionBy(\"index\")\n",
    "\n",
    "#calculating the Euclidean distance between Bart stations and weather stations\n",
    "#filtering by min distance from each bart station to weather stations \n",
    "station_weather_joined = df_station_full1.join(weather4.withColumnRenamed(\"LATITUDE\",\"latitude_wthr\")\\\n",
    "            .withColumnRenamed(\"LONGITUDE\",\"longitude_wthr\")) \\\n",
    "            .withColumn(\"distance\",f.sqrt(f.pow(f.col(\"latitude\")-f.col(\"latitude_wthr\"),2)+\\\n",
    "                                   f.pow(f.col(\"longitude\")-f.col(\"longitude_wthr\"),2)))\\\n",
    "            .withColumn(\"min_distance\", f.min(\"distance\").over(w))\\\n",
    "            .filter('distance=min_distance') \\\n",
    "            .drop('index','distance','min_distance','latitude','station_name','longitude')\n",
    "\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "82154e42-6ebc-44f1-b608-506b9b3a61b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 04:44:25 WARN TaskSetManager: Stage 359 contains a task of very large size (6273 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# writing to parquet\n",
    "# station_weather_joined.write.parquet('s3a://w210-bucket/data_wrangling/station_weather_cleanedUp.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "85e96ef6-6783-47f2-967f-ccbcbddade30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the station_weather_joined data\n",
    "station_weather_cleaned = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/station_weather_cleanedUp.parquet\")\n",
    "\n",
    "#getting rid of duplicate airport name\n",
    "station_weather_cleaned = station_weather_cleaned.filter(col('wthr_station_name') != 'SAN JOSE, CA US')\n",
    "#finding the closing the weather station and bart stations\n",
    "station_weather_closest = station_weather_cleaned.select('abbreviation','station').dropDuplicates()\n",
    "\n",
    "#joining station_weather_closest to the weather dataset\n",
    "\n",
    "final_weather = weather4.join(station_weather_closest,['station'],'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d85757ad-f47a-4006-bb83-dec1958df935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 04:44:53 WARN TaskSetManager: Stage 366 contains a task of very large size (6272 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# writing to parquet\n",
    "# final_weather.write.parquet('s3a://w210-bucket/data_wrangling/final_weather.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "35483704-5198-4117-af75-ccf1085d41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the station_weather_joined data\n",
    "final_weather = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/final_weather.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5cef73ec-b7bd-40f6-b606-563a0957b9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>ts</th>\n",
       "      <th>latitude_wthr</th>\n",
       "      <th>longitude_wthr</th>\n",
       "      <th>wthr_station_name</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wth_type</th>\n",
       "      <th>abbreviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station  ts  latitude_wthr  longitude_wthr  wthr_station_name  wind_speed  \\\n",
       "0        0   0              0               0                  0           0   \n",
       "\n",
       "   air_temp  precipitation  wth_type  abbreviation  \n",
       "0         0              0         0             0  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for nulls in station_weather_cleaned data\n",
    "missing_counts = final_weather.select([count(when(col(c).isNull(), c)).alias(c) for c in final_weather.columns]).toPandas()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "abb2234c-6e02-49a8-a3e8-6b75a8d948f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weather = final_weather.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "33a39b51-09d8-4538-a310-91348189f4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# # dropping duplicates from df_station_ridership and saving to parquet\n",
    "# df_station_ridership = df_station_ridership.dropDuplicates()\n",
    "# df_station_ridership.write.parquet('s3a://w210-bucket/data_wrangling/df_station_ridership.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0879ebd7-9877-4b87-a440-a20fd870f03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   wthr_station_name|\n",
      "+--------------------+\n",
      "|CONCORD BUCHANAN ...|\n",
      "|LIVERMORE MUNICIP...|\n",
      "|SAN JOSE INTERNAT...|\n",
      "|SAN FRANCISCO INT...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_weather.select('wthr_station_name').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3286792a-c7c5-4f21-9f19-9e9f26197ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the df_station_ridership data\n",
    "df_station_ridership = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_station_ridership.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7a020041-16fe-4922-b6d3-81e86d3d81df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- latitude_wthr: string (nullable = true)\n",
      " |-- longitude_wthr: string (nullable = true)\n",
      " |-- wthr_station_name: string (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- air_temp: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- wth_type: string (nullable = true)\n",
      " |-- abbreviation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_weather.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e85d0e70-bff3-4fe2-87d5-959996aceff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### joining station_weather dataset with the ridership dataset\n",
    "# # renaming the column to make it easier to join\n",
    "final_weather = final_weather.withColumnRenamed('abbreviation','origin')\n",
    "\n",
    "# df_joined1 = df_station_ridership.join(station_weather_cleaned1,(df_station_ridership['ts'] == station_weather_cleaned1['ts1']) & \\\n",
    "#                                        (df_station_ridership.origin == station_weather_cleaned1.abbreviation),'left').drop('ts1','abbreviation')\n",
    "\n",
    "df_joined1 = df_station_ridership.join(final_weather,['origin','ts'],'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7f97e82f-a1ee-472f-9bea-962d3859efcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#writing to parquet\n",
    "# df_joined1.write.parquet('s3a://w210-bucket/data_wrangling/df_joined1.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "051e7ea4-aff0-4139-94c9-270b857e76ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>ts</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>destination</th>\n",
       "      <th>ridership_number</th>\n",
       "      <th>origin-des</th>\n",
       "      <th>station_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>quarter</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>station</th>\n",
       "      <th>latitude_wthr</th>\n",
       "      <th>longitude_wthr</th>\n",
       "      <th>wthr_station_name</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wth_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "      <td>261669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin  ts  date  hour  destination  ridership_number  origin-des  \\\n",
       "0       0   0     0     0            0                 0           0   \n",
       "\n",
       "   station_name  longitude  latitude  ...  quarter  day_of_week  station  \\\n",
       "0             0          0         0  ...        0            0   261669   \n",
       "\n",
       "   latitude_wthr  longitude_wthr  wthr_station_name  wind_speed  air_temp  \\\n",
       "0         261669          261669             261669      261669    261669   \n",
       "\n",
       "   precipitation  wth_type  \n",
       "0         261669    261669  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading df_joined1 and counting missing values \n",
    "df_joined1 = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_joined1.parquet\")\n",
    "\n",
    "# checking for nulls in df_joined1 data\n",
    "missing_counts = df_joined1.select([count(when(col(c).isNull(), c)).alias(c) for c in df_joined1.columns]).toPandas()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8d3d766-1451-4f3c-bdaf-cfdb4200e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping null values. About 0.24% of the data\n",
    "df_joined1 = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_joined1.parquet\")\n",
    "df_joined1 = df_joined1.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbae1368-3712-4250-a4ce-681d1f8ca9e7",
   "metadata": {},
   "source": [
    "### joining weather based on destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7280a3e9-48e1-4da5-9510-43f12bc2c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming weather columns for the origin\n",
    "df_joined1_renamed = df_joined1.withColumnRenamed('station', 'station_origin')\\\n",
    "                       .withColumnRenamed('wthr_station_name', 'wthr_station_origin')\\\n",
    "                       .withColumnRenamed('latitude_wthr', 'latitude_wthr_origin')\\\n",
    "                       .withColumnRenamed('longitude_wthr', 'longitude_wthr_origin')\\\n",
    "                       .withColumnRenamed('wind_speed', 'wind_speed_origin')\\\n",
    "                       .withColumnRenamed('air_temp', 'air_temp_origin')\\\n",
    "                       .withColumnRenamed('precipitation', 'precipitation_origin')\\\n",
    "                       .withColumnRenamed('wth_type', 'wth_type_origin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f174fb5-7720-4467-8d27-4d96a04a6a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- origin: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      " |-- ridership_number: integer (nullable = true)\n",
      " |-- origin-des: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- quarter: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- station_origin: string (nullable = true)\n",
      " |-- latitude_wthr_origin: string (nullable = true)\n",
      " |-- longitude_wthr_origin: string (nullable = true)\n",
      " |-- wthr_station_origin: string (nullable = true)\n",
      " |-- wind_speed_origin: double (nullable = true)\n",
      " |-- air_temp_origin: double (nullable = true)\n",
      " |-- precipitation_origin: double (nullable = true)\n",
      " |-- wth_type_origin: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined1_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4b9a158-c70e-4f51-a03b-a98f8bcab149",
   "metadata": {},
   "outputs": [],
   "source": [
    "### joining station_weather dataset with the ridership dataset\n",
    "# # renaming the column to make it easier to join\n",
    "\n",
    "# reading the final_weather data\n",
    "final_weather = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/final_weather.parquet\")\n",
    "\n",
    "#dropping duplicates \n",
    "final_weather = final_weather.dropDuplicates()\n",
    "\n",
    "final_weather = final_weather.withColumnRenamed('abbreviation','destination')\n",
    "\n",
    "df_joined2 = df_joined1_renamed.join(final_weather,['destination','ts'],'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3b65cd9-6d44-44be-b5e1-922146b62e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 05:13:29 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving to S3 as a parquet file \n",
    "# df_joined2.write.parquet('s3a://w210-bucket/data_wrangling/df_joined2.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d175ec36-1019-4e72-9764-dfa5ebe336d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the first joined data\n",
    "df_joined2 = spark.read.parquet(\"s3a://w210-bucket/data_wrangling/df_joined2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "570da6d3-ceaa-4e89-a3fa-4493d3e25f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- destination: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- ridership_number: integer (nullable = true)\n",
      " |-- origin-des: string (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- quarter: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- station_origin: string (nullable = true)\n",
      " |-- latitude_wthr_origin: string (nullable = true)\n",
      " |-- longitude_wthr_origin: string (nullable = true)\n",
      " |-- wthr_station_origin: string (nullable = true)\n",
      " |-- wind_speed_origin: double (nullable = true)\n",
      " |-- air_temp_origin: double (nullable = true)\n",
      " |-- precipitation_origin: double (nullable = true)\n",
      " |-- wth_type_origin: string (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- latitude_wthr: string (nullable = true)\n",
      " |-- longitude_wthr: string (nullable = true)\n",
      " |-- wthr_station_name: string (nullable = true)\n",
      " |-- wind_speed: double (nullable = true)\n",
      " |-- air_temp: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- wth_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99bb7b77-346c-4cba-aef9-f57c716234c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109442517"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1118f05-cfc5-4e9c-8d2c-e11ac7627b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination</th>\n",
       "      <th>ts</th>\n",
       "      <th>origin</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>ridership_number</th>\n",
       "      <th>origin-des</th>\n",
       "      <th>station_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>precipitation_origin</th>\n",
       "      <th>wth_type_origin</th>\n",
       "      <th>station</th>\n",
       "      <th>latitude_wthr</th>\n",
       "      <th>longitude_wthr</th>\n",
       "      <th>wthr_station_name</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wth_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "      <td>226936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   destination  ts  origin  date  hour  ridership_number  origin-des  \\\n",
       "0            0   0       0     0     0                 0           0   \n",
       "\n",
       "   station_name  longitude  latitude  ...  precipitation_origin  \\\n",
       "0             0          0         0  ...                     0   \n",
       "\n",
       "   wth_type_origin  station  latitude_wthr  longitude_wthr  wthr_station_name  \\\n",
       "0                0   226936         226936          226936             226936   \n",
       "\n",
       "   wind_speed  air_temp  precipitation  wth_type  \n",
       "0      226936    226936         226936    226936  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for nulls in df_joined1 data\n",
    "missing_counts = df_joined2.select([count(when(col(c).isNull(), c)).alias(c) for c in df_joined2.columns]).toPandas()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47a4fbf6-eb04-4f39-a74f-c40f8b31be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined2_1 = df_joined2.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfb6d199-0758-4d07-8b18-ffde38821f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination</th>\n",
       "      <th>ts</th>\n",
       "      <th>origin</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>ridership_number</th>\n",
       "      <th>origin-des</th>\n",
       "      <th>station_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>precipitation_origin</th>\n",
       "      <th>wth_type_origin</th>\n",
       "      <th>station</th>\n",
       "      <th>latitude_wthr</th>\n",
       "      <th>longitude_wthr</th>\n",
       "      <th>wthr_station_name</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>wth_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   destination  ts  origin  date  hour  ridership_number  origin-des  \\\n",
       "0            0   0       0     0     0                 0           0   \n",
       "\n",
       "   station_name  longitude  latitude  ...  precipitation_origin  \\\n",
       "0             0          0         0  ...                     0   \n",
       "\n",
       "   wth_type_origin  station  latitude_wthr  longitude_wthr  wthr_station_name  \\\n",
       "0                0        0              0               0                  0   \n",
       "\n",
       "   wind_speed  air_temp  precipitation  wth_type  \n",
       "0           0         0              0         0  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for nulls in df_joined1 data\n",
    "missing_counts = df_joined2_1.select([count(when(col(c).isNull(), c)).alias(c) for c in df_joined2_1.columns]).toPandas()\n",
    "missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e49ca11-849d-4765-aa8c-8741550a6775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "109215581"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined2_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69c58d8c-a6c7-483b-9d8c-d088ac7a8b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#getting rid of nulls and resaving the file\n",
    "# df_joined2_1.write.parquet('s3a://w210-bucket/data_wrangling/df_joined2_1.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c41694-dd58-4160-a42b-0d13df8139a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
